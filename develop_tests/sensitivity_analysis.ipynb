{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global sensitivity Analysis for PULPO\n",
    "\n",
    "In this notebook we show the workflow for the global sensitivity analysis (GSA) in PULPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from pulpo import pulpo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining the case\n",
    "\n",
    "The case for which the sensitivity analysis will be performed for is on a solution of the LP:\n",
    "\n",
    "$$\n",
    "    \\begin{align}\n",
    "        & \\underset{s, slack}{\\text{min}}  && z_h \\\\\n",
    "        & \\text{s.t.}   && \\sum_{j}(a_{i,j}\\cdot s_j) = f_i && \\forall i \\\\\n",
    "        &               && s_j^{low} \\leq s_j \\leq s_j^{high} && \\forall j \\\\\n",
    "        &               && z_h = \\sum_e \\sum_j (q_{h,e}\\cdot b_{e,j} \\cdot s_j) && \\forall h \\\\\n",
    "    \\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Creating the problem \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the rise husk database has not been installed yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulpo.install_rice_husk_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters for the rise husk example to instancialize PULPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"rice_husk_example\" \n",
    "database = \"rice_husk_example_db\"\n",
    "method = \"('my project', 'climate change')\"\n",
    "notebook_dir = os.path.dirname(os.getcwd())\n",
    "directory = os.path.join(notebook_dir, 'develop_tests/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a **PulpoOptimizer** instance. This class is used to interact with the LCI database and solve the optimization problem. It is specified by the project, database, method and directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulpo_worker = pulpo.PulpoOptimizer(project, database, method, directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import LCI data. After initializing the PulpoOptimizer instance, the LCI data is imported from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulpo_worker.get_lci_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the **functional unit**. In this case, the functional unit is 1 Mt of processed rice. PULPO implements a search function (```retrieve_processes```) to find the processes that match the specified reference products (alternatively: keys, process name, region)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rice_factory = pulpo_worker.retrieve_processes(reference_products='Processed rice (in Mt)')\n",
    "\n",
    "demand = {rice_factory[0]: 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the **choices**. Here, the choices are regional üåê choices for rise husk collections, and technological ‚õè choices for boiler type selection.\n",
    "\n",
    "The auxiliar choices are needed to resolve the issue that rice, when not used in the boiler must be burned instead. \n",
    "\n",
    "(*At this point, just accept. If you are curious about how this multi-functionality is technically adressed, refer to the paper, or reach out.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rise husk collection\n",
    "rice_husk_processes = [\"Rice husk collection 1\",\n",
    "              \"Rice husk collection 2\",\n",
    "              \"Rice husk collection 3\",\n",
    "              \"Rice husk collection 4\",\n",
    "              \"Rice husk collection 5\",]\n",
    "rice_husk_collections = pulpo_worker.retrieve_processes(processes=rice_husk_processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Boilers\n",
    "boiler_processes = [\"Natural gas boiler\",\n",
    "                     \"Wood pellet boiler\",\n",
    "                     \"Rice husk boiler\"]\n",
    "boilers = pulpo_worker.retrieve_processes(processes=boiler_processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Auxiliar (Ignore for now!)\n",
    "auxiliar_processes = [\"Rice husk market\",\n",
    "                       \"Burning of rice husk\"]\n",
    "auxiliar = pulpo_worker.retrieve_processes(processes=auxiliar_processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine to create the choices dictionary\n",
    "## For each kind of choice, assign a 'label' (e.g. 'boilers')\n",
    "## To each possible choice, assign a process capacity. In the 'unconstrained' case, set this value very high (e.g. 1e10, but depends on the scale of the functional unit)\n",
    "choices = {'Rice Husk (Mt)': {rice_husk_collections[0]: 0.03,\n",
    "                              rice_husk_collections[1]: 0.03,\n",
    "                              rice_husk_collections[2]: 0.03,\n",
    "                              rice_husk_collections[3]: 0.03,\n",
    "                              rice_husk_collections[4]: 0.03},\n",
    "           'Thermal Energy (TWh)': {boilers[0]: 1e10,\n",
    "                                    boilers[1]: 1e10,\n",
    "                                    boilers[2]: 1e10},\n",
    "           'Auxiliar': {auxiliar[0]: 1e10,\n",
    "                        auxiliar[1]: 1e10}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiate** and **solve** the optimization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulpo_worker.instantiate(choices=choices, demand=demand)\n",
    "results = pulpo_worker.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyomo.environ as pyo\n",
    "from pyomo.repn.plugins.baron_writer import *\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "def extract_results(instance, project, database, choices, constraints, demand, process_map, process_map_metadata, itervention_map, itervention_map_metadata, directory, name):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        instance: The Pyomo model instance.\n",
    "        project (str): Name of the project.\n",
    "        database (str): Name of the database.\n",
    "        choices (dict): Choices for the model.\n",
    "        constraints (dict): Constraints applied during optimization.\n",
    "        demand (dict): Demand data used in optimization.\n",
    "        process_map (dict): Mapping of process IDs to descriptions.\n",
    "        process_map_metadata (dict): Metadata to the process_map\n",
    "        itervention_map (dict): Mapping of intervention IDs to descriptions.\n",
    "        itervention_map_metadata (dict): Metadata of the itervention_map.\n",
    "        directory (str): Directory to save the results file.\n",
    "        name (str): Name of the results file.\n",
    "    \"\"\"\n",
    "    # Recover dictionary values\n",
    "    list_of_vars = []\n",
    "    for v in instance.component_objects(ctype=pyo.Var, active=True, descend_into=True):\n",
    "        for e in v._data:\n",
    "            v._data[e] = value(v[e])\n",
    "        list_of_vars.append(v)\n",
    "\n",
    "    result_data = {}\n",
    "    inverse_process_map = dict((v, k) for k, v in process_map.items())\n",
    "    inverse_itervention_map = dict((v, k) for k, v in itervention_map.items())\n",
    "    # Raw results\n",
    "    for v in list_of_vars:\n",
    "        try:\n",
    "            if str(v) == 'inv_flows' or str(v) == 'inv_vector':\n",
    "                data = [(k, inverse_itervention_map[k], itervention_map_metadata[k], v) for k, v in v._data.items()]\n",
    "            else:\n",
    "                data = [(k, inverse_process_map[k], process_map_metadata[k], v) for k, v in v._data.items()]\n",
    "            df = pd.DataFrame(data, columns=['ID', 'Process name', \"Process metadata\", 'Value'])\n",
    "        except:\n",
    "            data = [(k, v) for k, v in v._data.items()]\n",
    "            df = pd.DataFrame(data, columns=['Key', 'Value'])\n",
    "        df.sort_values(by=['Value'], inplace=True, ascending=False)\n",
    "        result_data[v.name] = df\n",
    "\n",
    "    # Normalize database to a list if it is a string\n",
    "    if isinstance(database, str):\n",
    "        database = [database]\n",
    "\n",
    "    # Store the metadata\n",
    "    result_data[\"project and db\"] = pd.DataFrame([f\"{project}__{db}\" for db in database])\n",
    "\n",
    "    choices_data = {}\n",
    "    for choice in choices:\n",
    "        i = 0\n",
    "        temp_dict = []\n",
    "        for alt in choices[choice]:\n",
    "            temp_dict.append((alt, i, instance.scaling_vector[process_map[alt.key]]))\n",
    "            i+=1\n",
    "        choices_data[(choice, 'Process')] = {'Process ' + str(i): process_map_metadata[process_map[alt.key]] for alt, i, val in temp_dict}\n",
    "        choices_data[(choice, 'Capacity')] = {'Process ' + str(i): choices[choice][alt] for alt, i, val in temp_dict}\n",
    "        choices_data[(choice, 'Value')] = {'Process ' + str(i): x for alt, i, x in temp_dict}\n",
    "    result_data[\"choices\"] = pd.DataFrame(choices_data)\n",
    "\n",
    "    result_data[\"demand\"] = pd.DataFrame({\"demand\":{\n",
    "        process_map_metadata[process_map[key]] if key in process_map else key: demand[key]\n",
    "        for key in demand\n",
    "    }})\n",
    "    result_data[\"constraints\"] = pd.DataFrame({\"Demand\": {process_map_metadata[process_map[key]]: constraints[key] for key in constraints}})\n",
    "\n",
    "    return result_data\n",
    "\n",
    "def save_results(result_data, file_name):\n",
    "    with pd.ExcelWriter(f\"{directory}/results/{file_name}.xlsx\") as writer:\n",
    "        for sheet_name, dataframe in result_data.items():\n",
    "            dataframe.to_excel(writer, sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save** and **summarize** the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data = extract_results(pulpo_worker.instance, pulpo_worker.project, pulpo_worker.database, choices, {}, demand,\n",
    "                            pulpo_worker.lci_data['process_map'], pulpo_worker.lci_data['process_map_metadata'],\n",
    "                            pulpo_worker.lci_data['intervention_map'], pulpo_worker.lci_data['intervention_map_metadata'],\n",
    "                            pulpo_worker.directory, \"\")\n",
    "result_data\n",
    "# pulpo_worker.summarize_results(choices=choices, demand=demand, zeroes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(result_data, \"new_test_extract_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the scaling vector from the results $s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_solution_df = result_data[\"scaling_vector\"][\"Value\"]\n",
    "s_solution_df.index = pd.MultiIndex.from_tuples(result_data[\"scaling_vector\"][\"Process name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparing the data for the sensitivity analysis\n",
    "\n",
    "**Reformulating the problem for the sensitivity analysis**\n",
    "\n",
    "We only consider uncertainty in the $B$ and $Q$ parameter matrizes. The scaling vector is given by the optimal solution.\n",
    "\n",
    "We will look at the environmental impact objective:\n",
    "\n",
    "$$\n",
    "    e(Q, B) =  Q \\cdot B \\cdot s\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Preparing the sampling of the parameter space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. Extracting the intervention and characterization matrizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the intervention matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervention_matrix_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "    pulpo_worker.lci_data[\"intervention_matrix\"], \n",
    "    index=pulpo_worker.lci_data[\"intervention_map\"],\n",
    "    columns=pulpo_worker.lci_data[\"process_map\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the characterization matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a fake multi method case for testing Q\n",
    "key = list(pulpo_worker.lci_data[\"matrices\"].keys())[0]\n",
    "Q_dict = pulpo_worker.lci_data[\"matrices\"].copy()\n",
    "Q_dict[\"copy \"+key] = pulpo_worker.lci_data[\"matrices\"][key]\n",
    "Q_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characterization_matrix_dfs = []\n",
    "for method, characterization_matrix in Q_dict.items():\n",
    "    method_index = {((method,) + intervention_flow):index for intervention_flow, index in pulpo_worker.lci_data[\"intervention_map\"].items()}\n",
    "    characterization_matrix_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "        characterization_matrix,\n",
    "        index=method_index,\n",
    "        columns=pulpo_worker.lci_data[\"intervention_map\"]\n",
    "    )\n",
    "    characterization_matrix_dfs.append(characterization_matrix_df)\n",
    "multi_characterization_matrix_df = pd.concat(characterization_matrix_dfs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Filtering out the Biosphereflows $B_{i,j}$ that have a neglectable impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate $Q\\cdot B$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_cost_matrix_df = multi_characterization_matrix_df.dot(intervention_matrix_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate $Q\\cdot B \\cdot s$ as matrix products, to get the impact of each $B_{i,j}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impacts_matrix_df = env_cost_matrix_df.mul(s_solution_df, axis=1)\n",
    "impacts_matrix_df_stacked = impacts_matrix_df.melt(ignore_index=False)\n",
    "melted_index = pd.MultiIndex.from_frame(\n",
    "    pd.concat(\n",
    "        [\n",
    "            impacts_matrix_df_stacked.index.to_frame(), \n",
    "            impacts_matrix_df_stacked[['variable_0','variable_1']]\n",
    "        ], \n",
    "        axis=1\n",
    "        )\n",
    ")\n",
    "impacts_matrix_df_stacked = impacts_matrix_df_stacked.drop(\n",
    "    columns=['variable_0','variable_1']\n",
    "    ).set_index(melted_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the reduced set of intervention flows which result in an absolute $QBs_{i,j}$ value lower then $10^n$, where $n$ is found out iteratively, by seeing how much the total impact changes when removing the intervention flows with $\\text{abs}(QBs) < 10^n$. The total impact should not change more then 1\\%.\n",
    "\n",
    "This is done for every impact category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total impact of the QBs with all biosphere flows is:\")\n",
    "for method, impact_value in impacts_matrix_df_stacked.groupby(level=0).sum().sum(axis=1).items():\n",
    "    print(\"{}: {:.6E}\".format(method, impact_value))\n",
    "# droplevel means dropping the method descriptor and unique takes away all duplicate invervention flows since the intervention flows have been joined on the methods\n",
    "print(\"With {} biosphere flows non equal to zero\".format(impacts_matrix_df_stacked[impacts_matrix_df_stacked != 0].dropna().index.droplevel(0).unique().shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_B = -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impacts_matrix_red_list = []\n",
    "for method, impacts_matrix in impacts_matrix_df_stacked.groupby(level=0):\n",
    "    impacts_matrix_gr0 = impacts_matrix[impacts_matrix != 0].dropna()\n",
    "    impacts_matrix_log10 = np.log10(impacts_matrix_gr0.abs())\n",
    "    print(\"The total impact of the reduced QBs is:\")\n",
    "    impacts_matrix_red = impacts_matrix_gr0[impacts_matrix_log10>n_B].dropna()\n",
    "    total_QBs_red = impacts_matrix_red.sum().values[0]\n",
    "    print(\"{:.6E}\".format(total_QBs_red))\n",
    "    print(\"This results in a percentage change of the total impact:\")\n",
    "    print(\"{:.2}%\".format(100-total_QBs_red/impacts_matrix.sum().sum()*100))\n",
    "    print(\"With {} biosphere flows\".format(impacts_matrix_red.shape[0]))\n",
    "    method\n",
    "    impacts_matrix_red_list.append(impacts_matrix_red)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concat the impact matrizes of the different impact categories again, for future strealined computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_impact_matrix_reduced = pd.concat(impacts_matrix_red_list, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce the intervention matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Keep only the internvention flows which have a reasonable impact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stacked intervention matrix and retain all levels of multiindex\n",
    "intervention_matrix_df_stacked = intervention_matrix_df.melt(ignore_index=False)\n",
    "melted_index = pd.MultiIndex.from_frame(\n",
    "    pd.concat(\n",
    "        [\n",
    "            intervention_matrix_df_stacked.index.to_frame(), \n",
    "            intervention_matrix_df_stacked[['variable_0','variable_1']]\n",
    "        ], \n",
    "        axis=1\n",
    "        )\n",
    ")\n",
    "intervention_matrix_df_stacked = intervention_matrix_df_stacked[[\"value\"]].set_index(melted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the interventions flows which still are in the reduced impact stacked matrix\n",
    "interventions_in_reduced_impact_matrix = multi_impact_matrix_reduced.index.droplevel(0).unique() # Drop the method name, now the index is the same as stacked intervention matrix\n",
    "intervention_matrix_df_stacked_reduced = intervention_matrix_df_stacked.loc[interventions_in_reduced_impact_matrix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Filtering out the unused characterization matrix elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack the characterization matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_characterization_matrix_df_stacked = multi_characterization_matrix_df.melt(ignore_index=False)\n",
    "multi_characterization_matrix_df_stacked = multi_characterization_matrix_df_stacked[[\"value\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Keep only the charactarization factors $Q_i$ for which there still are biosphere flows (Many biosphere flows of one kind are dropped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the process database and process name, now the index is the same as stacked characterization matrix\n",
    "characterization_factors_in_reduced_impact_matrix = multi_impact_matrix_reduced.index.droplevel(['variable_0', 'variable_1']).unique()\n",
    "multi_characterization_matrix_df_stacked_reduced = multi_characterization_matrix_df_stacked.loc[characterization_factors_in_reduced_impact_matrix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Drop all characterization factors equal to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_characterization_matrix_df_stacked_reduced = multi_characterization_matrix_df_stacked_reduced[multi_characterization_matrix_df_stacked_reduced != 0].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Getting the standard deviation of the parameter values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Set the uncertainty concept for the parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invented uncertainty factors for testing\n",
    "uncertainty_factors = {\n",
    "    \"matrizes\" : {\n",
    "        \"('my project', 'climate change')\": 0.1,\n",
    "        \"copy ('my project', 'climate change')\": 0.5\n",
    "    },\n",
    "    \"intervention_matrix\": 0.15\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Calculte the sigma values of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_multi_characterization_matrix_df = multi_characterization_matrix_df_stacked_reduced.mul(pd.Series(uncertainty_factors['matrizes']), axis=0, level=0).abs()\n",
    "sigma_intervention_matrix = intervention_matrix_df_stacked_reduced.mul(uncertainty_factors[\"intervention_matrix\"]).abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define the global sensitivity problem\n",
    "### 4.1. Define the bound/interval of the parameters\n",
    "Defining the bound as plus minus the standard deviation, $\\sigma^2$, of the nominal value, $\\mu$, for each parameter, $p$:\n",
    "\n",
    "$[\\mu_p - \\sigma^2_p; \\mu_p + \\sigma^2_p]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_intervention_matrix = intervention_matrix_df_stacked_reduced - sigma_intervention_matrix\n",
    "ub_intervention_matrix = intervention_matrix_df_stacked_reduced + sigma_intervention_matrix\n",
    "bounds_intervention_matrix = pd.concat([lb_intervention_matrix, ub_intervention_matrix], axis=1)\n",
    "bound_B = bounds_intervention_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_characterization_matrix = multi_characterization_matrix_df_stacked_reduced - sigma_multi_characterization_matrix_df\n",
    "ub_characterization_matrix = multi_characterization_matrix_df_stacked_reduced + sigma_multi_characterization_matrix_df\n",
    "bounds_characterization_matrix = pd.concat([lb_characterization_matrix, ub_characterization_matrix], axis=1)\n",
    "bound_Q = bounds_characterization_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Defining the sampling problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_QB = {\n",
    "    'num_vars': multi_characterization_matrix_df_stacked_reduced.shape[0] + intervention_matrix_df_stacked_reduced.shape[0],\n",
    "    'names': multi_characterization_matrix_df_stacked_reduced.index.to_list() + intervention_matrix_df_stacked_reduced.index.to_list(),\n",
    "    'bounds': bounds_characterization_matrix.values.tolist() + bounds_intervention_matrix.values.tolist()\n",
    "}\n",
    "problem_QB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Define the sampling method\n",
    "\n",
    "Select the sampling method, any sampling method from SALib can be chosen, but for most sensitivity analysis there is a sampling method best suited or even neccessary, in this case we use `saltelli` sampling because it is best compatible with Sobol' sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SALib.sample import saltelli as sample_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the amount of samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2**7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Define the sensitivity analysis method\n",
    "\n",
    "Select the sensitivity analysis method, remember it is mostly coupled to the sampling method, any sensitivity method from the SALib can be chosen. For this case study we chose Sobol' method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SALib.analyze import sobol as SA_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Perform the sensitivity analysis\n",
    "\n",
    "$$\n",
    "    e(Q, B) =  Q \\cdot B \\cdot s\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Sampling the $Q$ and $B$ arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_QB = sample_method.sample(problem_QB, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Solving for $Q \\cdot B \\cdot s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract matrizes from sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_Q = pd.DataFrame(sample_data_QB[:,:bound_Q.shape[0]], columns=bound_Q.index)\n",
    "sample_B = pd.DataFrame(sample_data_QB[:,bound_Q.shape[0]:], columns=bound_B.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manipulate the columns header and names to be able to perform matrix operations on the complete sample at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the invervention matrix columns headers\n",
    "sample_B_cols = sample_B.columns.to_frame().reset_index(drop=True).rename(columns={0:\"intervention_db\", 1:\"intervention_flow\"})\n",
    "sample_B.columns = pd.MultiIndex.from_frame(sample_B_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the characterization matrix columns headers\n",
    "sample_Q_cols = sample_Q.columns.to_frame().reset_index(drop=True).rename(columns={0:\"method\", 1:\"intervention_db\", 2:\"intervention_flow\"})\n",
    "sample_Q.columns = pd.MultiIndex.from_frame(sample_Q_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the characterizationa and intervention matrix columns\n",
    "merged_cols_B_left = pd.merge(left=sample_B_cols, right=sample_Q_cols, on = [\"intervention_db\",\"intervention_flow\"], how='inner')\n",
    "merged_cols_B_right = pd.merge(left=sample_Q_cols, right=sample_B_cols, on = [\"intervention_db\",\"intervention_flow\"], how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand the intervention matrix and the characterization matrix so we can perform dot products and get the impacts per biosphere flow and impact category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_Q_expanded = sample_Q.reindex(columns=pd.MultiIndex.from_frame(merged_cols_B_right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_B_expanded = sample_B.reindex(columns=pd.MultiIndex.from_frame(merged_cols_B_left))\n",
    "sample_B_expanded.columns = sample_B_expanded.columns.reorder_levels(sample_Q_expanded.columns.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the environmental cost matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_QB = sample_Q_expanded * sample_B_expanded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the environmental impact and group the the impacts based on their category for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QBs = {}\n",
    "multi_QB.columns = multi_QB.columns.reorder_levels(['variable_0', 'variable_1','method', 'intervention_db', 'intervention_flow'])\n",
    "for method, QB in multi_QB.groupby(level='method', axis=1):\n",
    "    s_for_QBs = s_solution_df.reindex(index=QB.columns)\n",
    "    QBs[method] = QB * s_for_QBs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Calculate the total output variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have multiple impact categories in the sampled data and we can only perform the sensitivity analysis per category, we have to specify the category from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_method = \"('my project', 'climate change')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_QBs = QBs[selected_method].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_QBs.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mu-sigma: {:.2E} \\t sigma: {:.2E} \\t mu+sigma: {:.2E}\".format(e_QBs.mean()-e_QBs.std(), e_QBs.mean(), e_QBs.mean()+e_QBs.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1.  Show the z-value and the distribution of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_QBs.plot.hist(bins=10)\n",
    "e_QBs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The z-value of the total environmental impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_QBs.std()/e_QBs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Calculate Sobol index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Si_QBs = SA_method.analyze(problem_QB, e_QBs.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_Si_QBs, first_Si_QBs, second_Si_QBs = Si_QBs.to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are currently performing one big sampling sampling including all impact categories, they will appear in the explained variance list, when we look at the impact of each impact category. They should always have zero or close to zero  sensitivity indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_Si_QBs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.1. Calculate total explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total explained variance is \\n{:.4}%\".format(total_Si_QBs[\"ST\"].sum()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5. Plot the contribution to variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the data and the names for the contribution plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_size(width, height, fraction=1):\n",
    "    \"\"\" Set aesthetic figure dimensions to avoid scaling in latex.\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    width: float\n",
    "            Width in pts\n",
    "    fraction: float\n",
    "            Fraction of the width which you wish the figure to occupy\n",
    " \n",
    "    Returns\n",
    "    -------\n",
    "    fig_dim: tuple\n",
    "            Dimensions of figure in inches\n",
    "    \"\"\"\n",
    "    # Width of figure\n",
    "    fig_width_pt = width * fraction    \n",
    " \n",
    "    # Convert from pt to inches\n",
    "    inches_per_pt = 1 / 72.27\n",
    " \n",
    "    # Golden ratio to set aesthetic figure height\n",
    "    golden_ratio = (5**.5 - 1) / 2\n",
    " \n",
    "    # Figure width in inches\n",
    "    fig_width_in = fig_width_pt * inches_per_pt\n",
    "    # Figure height in inches\n",
    "    if height: #if height is specified\n",
    "        fig_height_pt = height * fraction\n",
    "        fig_height_in = fig_height_pt * inches_per_pt\n",
    "    else:\n",
    "        fig_height_in = fig_width_in * golden_ratio\n",
    " \n",
    "    fig_dim = (fig_width_in, fig_height_in)\n",
    " \n",
    "    return fig_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import textwrap\n",
    "\n",
    "def plot_SA_barplot(data:pd.DataFrame, metadata:pd.DataFrame, colormap:pd.Series=pd.Series([]), bbox_to_anchor_lower:float = -0.6, bbox_to_anchor_center:float=0.5):\n",
    "    \"\"\"\n",
    "        Barplot of the contributional variance of the parameters in an objective\n",
    "\n",
    "        args:\n",
    "            data:       dataframe with columns: \"ST\" and \"ST_conf\"\n",
    "            metadata:   metadataframe with \"bar_names\" column and same indices as data\n",
    "            colormap:   Series with color codes to each data index     `colormap = pd.Series(mpl.cm.tab20.colors[:data.shape[0]], index=data.index)`\n",
    "            bbox_to_anchor_lower: negative float, scaled how much the legend is under the plot\n",
    "    \"\"\"\n",
    "    # width = 180\n",
    "    # height = 180\n",
    "    width = 4.77*72.4#600\n",
    "    height = None\n",
    "    _, ax = plt.subplots(1, 1, figsize=set_size(width,height))\n",
    "\n",
    "    # Data\n",
    "    data = data.sort_values([\"ST\"], ascending=False)\n",
    "    heights = data[\"ST\"].values * 100\n",
    "    yerrs = data[\"ST_conf\"].values * 100\n",
    "    bars = [textwrap.fill(string, 50) for string in metadata[\"bar_names\"].reindex(data.index)]\n",
    "    y_pos = range(len(bars))\n",
    "    \n",
    "    for height, y_po, yerr, indx in zip(heights, y_pos, yerrs, data.index):\n",
    "        ax.bar(y_po, height, yerr=yerr, capsize=5, ecolor=\"gray\", color=colormap[indx], alpha=0.9)\n",
    "    ax.set_xticks([])\n",
    "    if (data[\"ST\"]<=1).all() and (data[\"ST\"]>=0).all():\n",
    "        ax.yaxis.set_major_formatter(mpl.ticker.PercentFormatter())\n",
    "        ax.yaxis.set_major_locator(mpl.ticker.MultipleLocator(10))\n",
    "        # For the minor ticks, use no labels; default NullFormatter.\n",
    "        ax.yaxis.set_minor_locator(mpl.ticker.MultipleLocator(5))\n",
    "    ax.legend(bars, loc='lower center', bbox_to_anchor=(bbox_to_anchor_center, bbox_to_anchor_lower), borderpad=1)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.yaxis.grid(color='gray', linestyle='dotted')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_total_Si_QBs = total_Si_QBs.index.to_frame(name='bar_names')['bar_names'].agg(' - '.join).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = pd.Series(mpl.cm.tab20.colors[:total_Si_QBs.shape[0]], index=total_Si_QBs.index)\n",
    "plot_SA_barplot(data=total_Si_QBs, metadata=metadata_total_Si_QBs, colormap=colormap, bbox_to_anchor_center=1.7, bbox_to_anchor_lower=-.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6. Plot the main contributing variables to the total environmental impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QBs_per_s_sample = QBs[selected_method]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QBs_tops_indcs = QBs_per_s_sample.mean().abs().sort_values(ascending=False).iloc[:5].index\n",
    "data_QBs = pd.DataFrame([])\n",
    "QBs_per_s_sample_scaled = QBs_per_s_sample/ e_QBs.mean()#.divide(QBs_per_s_sample.abs().sum(axis=1), axis=\"index\")\n",
    "data_QBs[\"ST\"] = QBs_per_s_sample_scaled.mean()[QBs_tops_indcs]\n",
    "data_QBs[\"ST_conf\"] = QBs_per_s_sample_scaled.std()[QBs_tops_indcs]\n",
    "metadata_QBs = data_QBs.index.to_frame()\n",
    "metadata_QBs[\"bar_names\"] = [\"{} in {} \".format(metadata_QBs[\"intervention_flow\"], metadata_QBs[\"variable_1\"]) for _,meta in metadata_QBs.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the total environmental impact for the top processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "# set matplotlib colormap\n",
    "colormap_mpl = mpl.cm.Set3.colors\n",
    "\n",
    "def plot_total_env_impact_barplot(data:pd.DataFrame,  metadata:pd.DataFrame, impact_category:str, colormap:pd.Series=pd.Series([]), savefig:Optional[bool]=None, bbox_to_anchor_center:float=0.5):\n",
    "    \"\"\"\n",
    "        Barplot of the contributional variance of the parameters in total cost objective\n",
    "\n",
    "        args:\n",
    "            data:       dataframe with columns: \"ST\" and \"ST_conf\"\n",
    "            metadata:   metadataframe with \"bar_names\" column and same indices as data\n",
    "            impact_category:    name of environmental impact category\n",
    "            savefig:    if true saves fig into specified path\n",
    "    \"\"\"\n",
    "    bbox_to_anchor_lower = .7\n",
    "    bbox_to_anchor_center = .7\n",
    "    if colormap.empty:\n",
    "        colormap = pd.Series(colormap_mpl[:data.shape[0]], index=data.index)\n",
    "    else:\n",
    "        act_indcs = [index for index in colormap.index if type(index[1]) == int]\n",
    "        colormap_red = pd.Series(colormap[act_indcs].values, index=[indcs[1] for indcs in act_indcs])\n",
    "        addtional_incs = data.index[~data.index.isin(colormap_red.index)]\n",
    "        additional_colormap = pd.Series(\n",
    "            colormap_mpl[colormap.shape[0]:colormap.shape[0]+addtional_incs.shape[0]], \n",
    "            index = addtional_incs\n",
    "            )\n",
    "        colormap = pd.concat([colormap_red, additional_colormap])\n",
    "\n",
    "\n",
    "    ax = plot_SA_barplot(data, metadata, colormap=colormap, bbox_to_anchor_lower=bbox_to_anchor_lower, bbox_to_anchor_center=bbox_to_anchor_center)    \n",
    "    ax.set_xlabel(\"Main environmental parameters\")\n",
    "    ax.set_ylabel(\"Contribution to total {} in [\\%]\".format(impact_category))\n",
    "\n",
    "    # Save figure\n",
    "    if savefig:\n",
    "        plt.savefig(r\"C:\\Users\\admin\\OneDrive - Carbon Minds GmbH\\Dokumente\\13 Students\\MA_Bartolomeus_L√∂wgren\\02_code\\03_optimization_framework\\04_case_studies\\02_plots\\total_env_impact_barplot\" + \".{}\".format(fileformat), format=fileformat, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = pd.Series(mpl.cm.tab20.colors[:data_QBs.shape[0]], index=data_QBs.index)\n",
    "metadata_QBs = metadata_QBs[['bar_names']]\n",
    "plot_total_env_impact_barplot(data_QBs, metadata=metadata_QBs, impact_category=selected_method, savefig=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stacked intervention matrix and retain all levels of multiindex\n",
    "intervention_matrix_df_stacked = intervention_matrix_df.melt(ignore_index=False)\n",
    "melted_index = pd.MultiIndex.from_frame(\n",
    "    pd.concat(\n",
    "        [\n",
    "            intervention_matrix_df_stacked.index.to_frame(), \n",
    "            intervention_matrix_df_stacked[['variable_0','variable_1']]\n",
    "        ], \n",
    "        axis=1\n",
    "        )\n",
    ")\n",
    "intervention_matrix_df_stacked = intervention_matrix_df_stacked[[\"value\"]].set_index(melted_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pulpo_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
