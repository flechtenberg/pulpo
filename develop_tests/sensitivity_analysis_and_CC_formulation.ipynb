{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global sensitivity Analysis for PULPO\n",
    "\n",
    "In this notebook we show the workflow for the global sensitivity analysis (GSA) in PULPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from pulpo import pulpo\n",
    "import scipy.sparse as sparse\n",
    "from time import time\n",
    "import stats_arrays\n",
    "import scipy.stats\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_size(width, height, fraction=1):\n",
    "    \"\"\" Set aesthetic figure dimensions to avoid scaling in latex.\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    width: float\n",
    "            Width in pts\n",
    "    fraction: float\n",
    "            Fraction of the width which you wish the figure to occupy\n",
    " \n",
    "    Returns\n",
    "    -------\n",
    "    fig_dim: tuple\n",
    "            Dimensions of figure in inches\n",
    "    \"\"\"\n",
    "    # Width of figure\n",
    "    fig_width_pt = width * fraction    \n",
    " \n",
    "    # Convert from pt to inches\n",
    "    inches_per_pt = 1 / 72.27\n",
    " \n",
    "    # Golden ratio to set aesthetic figure height\n",
    "    golden_ratio = (5**.5 - 1) / 2\n",
    " \n",
    "    # Figure width in inches\n",
    "    fig_width_in = fig_width_pt * inches_per_pt\n",
    "    # Figure height in inches\n",
    "    if height: #if height is specified\n",
    "        fig_height_pt = height * fraction\n",
    "        fig_height_in = fig_height_pt * inches_per_pt\n",
    "    else:\n",
    "        fig_height_in = fig_width_in * golden_ratio\n",
    " \n",
    "    fig_dim = (fig_width_in, fig_height_in)\n",
    " \n",
    "    return fig_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyomo.environ as pyo\n",
    "from pyomo.repn.plugins.baron_writer import *\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "def extract_results(instance, project, database, choices, constraints, demand, process_map, process_map_metadata, itervention_map, itervention_map_metadata, directory, name):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        instance: The Pyomo model instance.\n",
    "        project (str): Name of the project.\n",
    "        database (str): Name of the database.\n",
    "        choices (dict): Choices for the model.\n",
    "        constraints (dict): Constraints applied during optimization.\n",
    "        demand (dict): Demand data used in optimization.\n",
    "        process_map (dict): Mapping of process IDs to descriptions.\n",
    "        process_map_metadata (dict): Metadata to the process_map\n",
    "        itervention_map (dict): Mapping of intervention IDs to descriptions.\n",
    "        itervention_map_metadata (dict): Metadata of the itervention_map.\n",
    "        directory (str): Directory to save the results file.\n",
    "        name (str): Name of the results file.\n",
    "    \"\"\"\n",
    "    # Recover dictionary values\n",
    "    # list_of_vars = []\n",
    "    # for v in instance.component_objects(ctype=pyo.Var, active=True, descend_into=True):\n",
    "    #     for e in v._data:\n",
    "    #         v._data[e] = value(v[e])\n",
    "    #     list_of_vars.append(v)\n",
    "\n",
    "    result_data = {}\n",
    "    # inverse_process_map = pd.DataFrame(dict((v, k) for k, v in process_map.items()))\n",
    "    # inverse_itervention_map = dict((v, k) for k, v in itervention_map.items())\n",
    "    process_map_df = pd.DataFrame.from_dict(process_map, orient='index', columns=['process_id']).reset_index(names='Process name').set_index('process_id')\n",
    "    process_map_df['Process metadata'] = process_map_df.index.map(process_map_metadata)\n",
    "    itervention_map_df = pd.DataFrame.from_dict(itervention_map, orient='index', columns=['intervention_id']).reset_index(names='Intervention name').set_index('intervention_id')\n",
    "    itervention_map_df['Invervention metadata'] = itervention_map_df.index.map(itervention_map_metadata)\n",
    "    # Raw results\n",
    "    for v in instance.component_objects(ctype=pyo.Var, active=True, descend_into=True):\n",
    "        df = pd.DataFrame.from_dict(v.get_values(), orient='index', columns=['Value']).sort_values('Value', ascending=False)\n",
    "        match v.name:\n",
    "            case 'inv_flows' | 'inv_vector':\n",
    "                df = df.join(itervention_map_df, how='left').reset_index(names='ID')          \n",
    "            case 'scaling_vector':\n",
    "                df = df.join(process_map_df, how='left').reset_index(names='ID')\n",
    "            case 'impacts' | 'slack' | 'impacts_calculated':\n",
    "                df = df.reset_index(names='Key')\n",
    "        result_data[v.name] = df\n",
    "\n",
    "    # Store the emvironmental costs\n",
    "    result_data[instance.ENV_COST_MATRIX.name] = pd.DataFrame.from_dict(\n",
    "        instance.ENV_COST_MATRIX.extract_values(), orient='index', \n",
    "        columns=[str(instance.ENV_COST_MATRIX)]\n",
    "        )\n",
    "\n",
    "    # Normalize database to a list if it is a string\n",
    "    if isinstance(database, str):\n",
    "        database = [database]\n",
    "\n",
    "    # Store the metadata\n",
    "    result_data[\"project and db\"] = pd.DataFrame([f\"{project}__{db}\" for db in database])\n",
    "\n",
    "    # ATTN: BHL: This needs to be rewritten, it is very convoluted and can be more clear\n",
    "    choices_data = {}\n",
    "    for choice, alternatives in choices.items():\n",
    "        temp_dict = []\n",
    "        for i_alt, alt in enumerate(alternatives):\n",
    "            temp_dict.append((alt, i_alt, instance.scaling_vector.get_values()[process_map[alt.key]]))\n",
    "        choices_data[(choice, 'Process')] = {f'Process {i}': process_map_metadata[process_map[alt.key]] for alt, i, _ in temp_dict}\n",
    "        choices_data[(choice, 'Capacity')] = {f'Process {i}': alternatives[alt] for alt, i, val in temp_dict}\n",
    "        choices_data[(choice, 'Value')] = {f'Process {i}': x for _, i, x in temp_dict}\n",
    "    result_data[\"choices\"] = pd.DataFrame(choices_data)\n",
    "\n",
    "    result_data[\"demand\"] = pd.DataFrame({\"demand\":{\n",
    "        process_map_metadata[process_map[key]] if key in process_map else key: demand[key]\n",
    "        for key in demand\n",
    "    }})\n",
    "    result_data[\"constraints\"] = pd.DataFrame({\"Demand\": {process_map_metadata[process_map[key]]: constraints[key] for key in constraints}})\n",
    "\n",
    "    return result_data\n",
    "\n",
    "def save_results(result_data, file_name, directory) -> None:\n",
    "    with pd.ExcelWriter(f\"{directory}/results/{file_name}.xlsx\") as writer:\n",
    "        for sheet_name, dataframe in result_data.items():\n",
    "            dataframe.to_excel(writer, sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining the case\n",
    "\n",
    "The case for which the sensitivity analysis will be performed for is on a solution of the LP:\n",
    "\n",
    "$$\n",
    "    \\begin{align}\n",
    "        & \\underset{s, slack}{\\text{min}}  && z_h \\\\\n",
    "        & \\text{s.t.}   && \\sum_{j}(a_{i,j}\\cdot s_j) = f_i && \\forall i \\\\\n",
    "        &               && s_j^{low} \\leq s_j \\leq s_j^{high} && \\forall j \\\\\n",
    "        &               && z_h = \\sum_e \\sum_j (q_{h,e}\\cdot b_{e,j} \\cdot s_j) && \\forall h \\\\\n",
    "    \\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Rice husk problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1. Loading the the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the rise husk database has not been installed yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulpo.install_rice_husk_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters for the rise husk example to instancialize PULPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"rice_husk_example\" \n",
    "database = \"rice_husk_example_db\"\n",
    "method = \"('my project', 'climate change')\"\n",
    "notebook_dir = os.path.dirname(os.getcwd())\n",
    "directory = os.path.join(notebook_dir, 'develop_tests/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a **PulpoOptimizer** instance. This class is used to interact with the LCI database and solve the optimization problem. It is specified by the project, database, method and directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulpo_worker = pulpo.PulpoOptimizer(project, database, method, directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import LCI data. After initializing the PulpoOptimizer instance, the LCI data is imported from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulpo_worker.get_lci_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2. Defining the optimizaiton problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the **functional unit**. In this case, the functional unit is 1 Mt of processed rice. PULPO implements a search function (```retrieve_processes```) to find the processes that match the specified reference products (alternatively: keys, process name, region)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rice_factory = pulpo_worker.retrieve_processes(reference_products='Processed rice (in Mt)')\n",
    "\n",
    "demand = {rice_factory[0]: 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the **choices**. Here, the choices are regional 🌐 choices for rise husk collections, and technological ⛏ choices for boiler type selection.\n",
    "\n",
    "The auxiliar choices are needed to resolve the issue that rice, when not used in the boiler must be burned instead. \n",
    "\n",
    "(*At this point, just accept. If you are curious about how this multi-functionality is technically adressed, refer to the paper, or reach out.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rise husk collection\n",
    "rice_husk_processes = [\"Rice husk collection 1\",\n",
    "              \"Rice husk collection 2\",\n",
    "              \"Rice husk collection 3\",\n",
    "              \"Rice husk collection 4\",\n",
    "              \"Rice husk collection 5\",]\n",
    "rice_husk_collections = pulpo_worker.retrieve_processes(processes=rice_husk_processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Boilers\n",
    "boiler_processes = [\"Natural gas boiler\",\n",
    "                     \"Wood pellet boiler\",\n",
    "                     \"Rice husk boiler\"]\n",
    "boilers = pulpo_worker.retrieve_processes(processes=boiler_processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Auxiliar (Ignore for now!)\n",
    "auxiliar_processes = [\"Rice husk market\",\n",
    "                       \"Burning of rice husk\"]\n",
    "auxiliar = pulpo_worker.retrieve_processes(processes=auxiliar_processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine to create the choices dictionary\n",
    "## For each kind of choice, assign a 'label' (e.g. 'boilers')\n",
    "## To each possible choice, assign a process capacity. In the 'unconstrained' case, set this value very high (e.g. 1e10, but depends on the scale of the functional unit)\n",
    "choices = {'Rice Husk (Mt)': {rice_husk_collections[0]: 0.03,\n",
    "                              rice_husk_collections[1]: 0.03,\n",
    "                              rice_husk_collections[2]: 0.03,\n",
    "                              rice_husk_collections[3]: 0.03,\n",
    "                              rice_husk_collections[4]: 0.03},\n",
    "           'Thermal Energy (TWh)': {boilers[0]: 1e10,\n",
    "                                    boilers[1]: 1e10,\n",
    "                                    boilers[2]: 1e10},\n",
    "           'Auxiliar': {auxiliar[0]: 1e10,\n",
    "                        auxiliar[1]: 1e10}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3. Instantiat and solve the optimization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulpo_worker.instantiate(choices=choices, demand=demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pulpo_worker.solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4. Save and summarize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data = extract_results(pulpo_worker.instance, pulpo_worker.project, pulpo_worker.database, choices, {}, demand,\n",
    "                            pulpo_worker.lci_data['process_map'], pulpo_worker.lci_data['process_map_metadata'],\n",
    "                            pulpo_worker.lci_data['intervention_map'], pulpo_worker.lci_data['intervention_map_metadata'],\n",
    "                            pulpo_worker.directory, \"\")\n",
    "pulpo_worker.summarize_results(choices=choices, demand=demand, zeroes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(result_data, \"new_test_extract_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Defining the electricity showcase problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1. Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your brightway2 project, database and targeted impact category\n",
    "project = \"pulpo\"\n",
    "database = \"cutoff38\"\n",
    "methods = {\"('IPCC 2013', 'climate change', 'GWP 100a')\": 1,\n",
    "           \"('ReCiPe Endpoint (E,A)', 'resources', 'total')\": 0,\n",
    "           \"('ReCiPe Endpoint (E,A)', 'human health', 'total')\": 0,\n",
    "           \"('ReCiPe Endpoint (E,A)', 'ecosystem quality', 'total')\": 0,\n",
    "           \"('ReCiPe Midpoint (E) V1.13', 'ionising radiation', 'IRP_HE')\": 0}\n",
    "\n",
    "# Substitute with your working directory of choice\n",
    "notebook_dir = os.path.dirname(os.getcwd())\n",
    "directory = os.path.join(notebook_dir, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PULPO object\n",
    "pulpo_worker = pulpo.PulpoOptimizer(project, database, methods, directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve LCI data\n",
    "pulpo_worker.get_lci_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2. Define the optimization problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the electricity market\n",
    "activities = [\"market for electricity, high voltage\"]\n",
    "reference_products = [\"electricity, high voltage\"]\n",
    "locations = [\"DE\"]\n",
    "electricity_market = pulpo_worker.retrieve_activities(activities=activities,\n",
    "                                                      reference_products=reference_products,\n",
    "                                                      locations=locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the functional unit as demand dictionary\n",
    "demand = {electricity_market[0]: 1.28819e+11}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the choices\n",
    "activities = [\"electricity production, lignite\", \n",
    "             \"electricity production, hard coal\",\n",
    "             \"electricity production, nuclear, pressure water reactor\",\n",
    "             \"electricity production, wind, 1-3MW turbine, onshore\"]\n",
    "reference_products = [\"electricity, high voltage\"]\n",
    "locations = [\"DE\"]\n",
    "\n",
    "electricity_activities = pulpo_worker.retrieve_activities(activities=activities,\n",
    "                                                          reference_products=reference_products,\n",
    "                                                          locations=locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the choices dictionary\n",
    "choices  = {'electricity': {electricity_activities[0]: 1e16,\n",
    "                            electricity_activities[1]: 1e16,\n",
    "                            electricity_activities[2]: 1e16,\n",
    "                            electricity_activities[3]: 1e16}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3. Initialize and solve the optimization problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and solve the problem (here with HiGHS)\n",
    "instance = pulpo_worker.instantiate(choices=choices, demand=demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulpo_worker.solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4. Summarize and save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data = extract_results(pulpo_worker.instance, pulpo_worker.project, pulpo_worker.database, choices, {}, demand,\n",
    "                            pulpo_worker.lci_data['process_map'], pulpo_worker.lci_data['process_map_metadata'],\n",
    "                            pulpo_worker.lci_data['intervention_map'], pulpo_worker.lci_data['intervention_map_metadata'],\n",
    "                            pulpo_worker.directory, \"\")\n",
    "pulpo_worker.summarize_results(choices=choices, demand=demand, zeroes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Defining the Ammonia case study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1. Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"pulpo-ammonia\"\n",
    "databases = [\"nc-inventories-ei310-all\", \"ecoinvent-3.10-cutoff\"]\n",
    "methods = \"('IPCC 2021', 'climate change', 'GWP 100a, incl. H and bio CO2')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substitute with your working directory of choice\n",
    "notebook_dir = os.path.dirname(os.getcwd())\n",
    "directory = os.path.join(notebook_dir, 'data')\n",
    "\n",
    "# Substitute with your GAMS path\n",
    "# GAMS_PATH = r\"C:\\APPS\\GAMS\\win64\\40.1\\gams.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulpo_worker = pulpo.PulpoOptimizer(project, databases, methods, directory)\n",
    "pulpo_worker.intervention_matrix=\"ecoinvent-3.10-biosphere\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulpo_worker.get_lci_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2. Define the optimization problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices_biogas = [\n",
    "    \"anaerobic digestion of animal manure, with biogenic carbon uptake\",\n",
    "    \"anaerobic digestion of agricultural residues, with biogenic carbon uptake\",\n",
    "    \"treatment of sewage sludge by anaerobic digestion, cut-off with biogenic carbon uptake\",\n",
    "    \"treatment of industrial wastewater by anaerobic digestion, cut-off with biogenic carbon uptake\",\n",
    "    \"treatment of biowaste by anaerobic digestion, cut-off with biogenic carbon uptake\",\n",
    "    \"anaerobic digestion of sequential crop, with biogenic carbon uptake\"\n",
    "]\n",
    "\n",
    "choices_hydrogen = [\n",
    "    \"hydrogen production, biomass gasification\",\n",
    "    \"hydrogen production, biomass gasification, with CCS\",\n",
    "    \"hydrogen production, steam methane reforming of biomethane\",\n",
    "    \"hydrogen production, steam methane reforming of biomethane, with CCS\",\n",
    "    \"hydrogen production, steam methane reforming of natural gas, with CCS\",\n",
    "    \"hydrogen production, PEM electrolysis, green\",\n",
    "    \"green hydrogen\",\n",
    "    \"hydrogen production, plastics gasification\",\n",
    "    \"hydrogen production, plastics gasification, with CCS\"\n",
    "]\n",
    "\n",
    "choices_heat = [\n",
    "    \"heat from biomethane\",\n",
    "    \"heat from biomethane, with CCS\",\n",
    "    \"heat from hydrogen\",\n",
    "    \"heat from natural gas, with CCS\"\n",
    "]\n",
    "\n",
    "choices_ammonia = [\n",
    "    \"ammonia production, steam methane reforming of biomethane\",\n",
    "    \"ammonia production, steam methane reforming of biomethane, with CCS\",\n",
    "    \"ammonia production, steam methane reforming of natural gas, with CCS\",\n",
    "    \"ammonia production, from nitrogen and hydrogen\"\n",
    "]\n",
    "\n",
    "choices_biomethane = [\n",
    "    \"biogas upgrading to biomethane, chemical scrubbing\",\n",
    "    \"biogas upgrading to biomethane, chemical scrubbing w/ CCS\",\n",
    "    \"biogas upgrading to biomethane, membrane\",\n",
    "    \"biogas upgrading to biomethane, membrane w/ CCS\",\n",
    "    \"biogas upgrading to biomethane, pressure swing adsorption\",\n",
    "    \"biogas upgrading to biomethane, pressure swing adsorption w/ CCS\",\n",
    "    \"biogas upgrading to biomethane, water scrubbing\",\n",
    "    \"biogas upgrading to biomethane, water scrubbing w/ CCS\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve activities for each category\n",
    "biogas_activities = pulpo_worker.retrieve_activities(activities=choices_biogas)\n",
    "hydrogen_activities = pulpo_worker.retrieve_activities(activities=choices_hydrogen)\n",
    "heat_activities = pulpo_worker.retrieve_activities(activities=choices_heat)\n",
    "biomethane_activities = pulpo_worker.retrieve_activities(activities=choices_biomethane)\n",
    "\n",
    "ammonia_activities = pulpo_worker.retrieve_activities(activities=choices_ammonia)\n",
    "# Add BAU Ammonia from ecoinvent as choice\n",
    "ammonia_activities.append(pulpo_worker.retrieve_activities(reference_products=\"ammonia, anhydrous, liquid\", activities=\"ammonia production, steam reforming, liquid\", locations=\"RER w/o RU\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = {\n",
    "    \"biogas\": {x: 1e10 for x in biogas_activities},\n",
    "    \"hydrogen\": {x: 1e10 for x in hydrogen_activities},\n",
    "    \"heat\": {x: 1e10 for x in heat_activities},\n",
    "    \"biomethane\": {x: 1e10 for x in biomethane_activities},\n",
    "    \"ammonia\": {x: 1e10 for x in ammonia_activities},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ammonia_market = pulpo_worker.retrieve_activities(activities=\"new market for ammonia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand = {ammonia_market[0]: 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3. Initialize and solve the optimization problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = pulpo_worker.instantiate(choices=choices, demand=demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pulpo_worker.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data = extract_results(pulpo_worker.instance, pulpo_worker.project, pulpo_worker.database, choices, {}, demand,\n",
    "                            pulpo_worker.lci_data['process_map'], pulpo_worker.lci_data['process_map_metadata'],\n",
    "                            pulpo_worker.lci_data['intervention_map'], pulpo_worker.lci_data['intervention_map_metadata'],\n",
    "                            pulpo_worker.directory, \"\")\n",
    "# save_results(result_data, 'ammonia_case_study', os.path.join(notebook_dir, 'develop_tests/data'))\n",
    "pulpo.saver.summarize_results(result_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impact_barplot(data:pd.DataFrame, metadata:pd.DataFrame, impact_category:str, colormap:pd.Series=pd.Series([]), bbox_to_anchor_lower:float = -0.6, bbox_to_anchor_center:float=0.5):\n",
    "    \"\"\"\n",
    "        Barplot of the contributional variance of the parameters in an objective\n",
    "\n",
    "        args:\n",
    "            data:       series with impacts as values and processes as index\n",
    "            metadata:   metadataframe with bar_names and same indices as data\n",
    "            colormap:   Series with color codes to each data index     `colormap = pd.Series(mpl.cm.tab20.colors[:data.shape[0]], index=data.index)`\n",
    "            bbox_to_anchor_lower: negative float, scaled how much the legend is under the plot\n",
    "    \"\"\"\n",
    "    # width = 180\n",
    "    # height = 180\n",
    "    width = 4.77*72.4#600\n",
    "    height = None\n",
    "    _, ax = plt.subplots(1, 1, figsize=set_size(width,height))\n",
    "\n",
    "    # Data\n",
    "    # data = data.sort_values([\"impact\"], ascending=False)\n",
    "    heights = data.values\n",
    "    bars = [textwrap.fill(string, 50) for string in metadata.reindex(data.index)]\n",
    "    y_pos = range(len(bars))\n",
    "    \n",
    "    for height, y_po, indx in zip(heights, y_pos, data.index):\n",
    "        ax.bar(y_po, height, capsize=5, ecolor=\"gray\", color=colormap[indx], alpha=0.9)\n",
    "    ax.set_xticks([])\n",
    "    if (data<=1).all() and (data>=0).all():\n",
    "        ax.yaxis.set_major_formatter(mpl.ticker.PercentFormatter())\n",
    "        ax.yaxis.set_major_locator(mpl.ticker.MultipleLocator(10))\n",
    "        # For the minor ticks, use no labels; default NullFormatter.\n",
    "        ax.yaxis.set_minor_locator(mpl.ticker.MultipleLocator(5))\n",
    "    ax.legend(bars, loc='lower center', bbox_to_anchor=(bbox_to_anchor_center, bbox_to_anchor_lower), borderpad=1)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.yaxis.grid(color='gray', linestyle='dotted')\n",
    "    ax.set_xlabel(\"Main environmental parameters\")\n",
    "    ax.set_ylabel(\"Contribution to total {} in [\\%]\".format(impact_category))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filtering out negletable uncertain parameters\n",
    "\n",
    "**Reformulating the problem for the sensitivity analysis**\n",
    "\n",
    "We only consider uncertainty in the $B$ and $Q$ parameter matrizes. The scaling vector is given by the optimal solution.\n",
    "\n",
    "We will look at the environmental impact objective:\n",
    "\n",
    "$$\n",
    "    e(Q, B) =  Q \\cdot B \\cdot s\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0. Preparing the sampling of the parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = result_data['impacts']['Key'][0]\n",
    "characterization_matrix = pulpo_worker.lci_data[\"matrices\"][method]\n",
    "characterization_params = pulpo_worker.lci_data[\"characterization_params\"][method]\n",
    "print('chosen environmental impact method: {}'.format(method))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.1. Define the scaling vector for the subsequent analysis as the optimization results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the scaling vector returned from the optimization into the same order as the process map\n",
    "scaling_vector_series = result_data[\"scaling_vector\"].set_index('ID')['Value'].sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.2. Define the scaling vector for the subsequent analysis as a constructed demand vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a demand vector which includes all alternatives in the demand use use the corresponding scaling vector of that LCA for the subsequent GSA and preparation steps, the idea is to include all relevant processes in the LCIA calculation instead of just those chosen by the optimizer at on Pareto point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for product, alternatives in choices.items():\n",
    "    demand_amount = result_data['choices'][product][\"Value\"].sum()\n",
    "    for alternative in alternatives:\n",
    "        demand[alternative] = demand_amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the scaling vector for the set constructed demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bw2calc\n",
    "import ast\n",
    "method_tuple = ast.literal_eval(method)\n",
    "lca = bw2calc.LCA(demand, method_tuple)\n",
    "lca.lci()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map the scaling vector results of the LCI calculation back to the optimization results index structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_mapper_df = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame.from_dict(pulpo_worker.lci_data['process_map'], orient='index', columns=['opt_problem']),\n",
    "        pd.DataFrame.from_dict(lca.product_dict, orient='index', columns=['lca'])\n",
    "    ],\n",
    "    axis=1\n",
    ").set_index('opt_problem')\n",
    "reindex_supply_array_df = index_mapper_df.merge(pd.DataFrame(lca.supply_array,  columns=['supply_array']), how='left', left_on='lca', right_index=True)\n",
    "scaling_vector_series = reindex_supply_array_df['supply_array']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Compute the LCI and LCIA results per flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCI calculatiom\n",
    "count = len(pulpo_worker.lci_data[\"process_map\"])\n",
    "inventory = pulpo_worker.lci_data['intervention_matrix'] * \\\n",
    "sparse.spdiags([scaling_vector_series.values], [0], count, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCIA calculation\n",
    "characterized_inventory = \\\n",
    "characterization_matrix * inventory\n",
    "lca_score = characterized_inventory.sum()\n",
    "print('The total impact is: {:e}'.format(lca_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the highest contributing processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_df = pd.DataFrame(\n",
    "    characterized_inventory.sum(axis=0).T,\n",
    "    index=list(range(characterized_inventory.shape[1])),\n",
    "    columns=['impact']\n",
    ")\n",
    "impact_df['process name'] = impact_df.index.map(pulpo_worker.lci_data['process_map_metadata'])\n",
    "impact_df = impact_df.reindex(impact_df['impact'].abs().sort_values(ascending=False).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_df_red = impact_df.iloc[:10,:]\n",
    "impact_df_red['impact'] = impact_df_red['impact'] / impact_df['impact'].sum()\n",
    "colormap = pd.Series(mpl.cm.tab20.colors[:impact_df_red.shape[0]], index=impact_df_red.index)\n",
    "impact_barplot(impact_df_red['impact'], metadata=impact_df_red['process name'], impact_category=method, colormap=colormap,  bbox_to_anchor_center=1.7, bbox_to_anchor_lower=-.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Filtering out the Biosphereflows $B_{i,j}$ that have a neglectable impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATTN: Add a simple optimization loop to find the cutoff which results in an absolute change of around 1%\n",
    "cutoff = 0.00002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filters the biosphere flows\n",
    "start = time()\n",
    "print('Characterized inventory:', characterized_inventory.shape, characterized_inventory.nnz)\n",
    "finv = characterized_inventory.multiply(abs(characterized_inventory) > abs(lca_score*cutoff))\n",
    "print('Filtered characterized inventory:', finv.shape, finv.nnz)\n",
    "characterized_inventory_indices = list(zip(*finv.nonzero()))\n",
    "# Since if negative and positive characterized inventories are cut away the explained fraction (finv.sum() / lca_score) can also be greater than 1\n",
    "deviation_from_lca_score = abs(1 - finv.sum() / lca_score)\n",
    "print('Deviation from LCA score:', deviation_from_lca_score)\n",
    "print('BIOSPHERE {} filtering resulted in {} of {} exchanges ({}% of total impact) and took {} seconds.'.format(\n",
    "    characterized_inventory.shape,\n",
    "    finv.nnz,\n",
    "    characterized_inventory.nnz,\n",
    "    np.round(100-deviation_from_lca_score * 100, 2),\n",
    "    np.round(time() - start, 2),\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Filter out the characterization factors not used for the biosphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter characterization matrix\n",
    "characterization_matrix_ids = characterization_matrix.diagonal().nonzero()[0]\n",
    "reduced_characterization_matrix_ids = []\n",
    "for (bio_i, ex_i) in characterized_inventory_indices:\n",
    "    if bio_i in characterization_matrix_ids and bio_i not in reduced_characterization_matrix_ids:\n",
    "        reduced_characterization_matrix_ids.append(bio_i)\n",
    "print('CHARACTERIZATION MATRIX {} filtering resulted in {} of {} characterization factors'.format(\n",
    "    characterization_matrix.diagonal().shape,\n",
    "    len(reduced_characterization_matrix_ids),\n",
    "    len(characterization_matrix_ids)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Getting the standard deviation of the parameter values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Get the paramteric uncertainty of the biosphere parameters found in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervention_metadata_df = pd.DataFrame(pulpo_worker.lci_data['intervention_params'])\n",
    "intervention_metadata_df = intervention_metadata_df.set_index([\"row\", \"col\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervention_flows_defined_uncertainty = {}\n",
    "intervention_flows_undefined_uncertainty = []\n",
    "for i_bug, characterized_inventory_index in enumerate(characterized_inventory_indices):\n",
    "    intervention_metadata = intervention_metadata_df.loc[characterized_inventory_index]\n",
    "    if intervention_metadata['uncertainty_type']>0: # if the uncertainty type is larger than 0, meaining there is an uncertainty defined\n",
    "        intervention_flows_defined_uncertainty[characterized_inventory_index] = intervention_metadata.to_dict()\n",
    "    else: # if no uncertainty data is available\n",
    "        intervention_flows_undefined_uncertainty.append(characterized_inventory_index)\n",
    "print(\"Intervention flows with uncertainty information: {} \\nIntervention flows without uncertainty information: {}\".format(len(intervention_flows_defined_uncertainty), len(intervention_flows_undefined_uncertainty)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Get the paramteric uncertainty of the characterizaiton factors found in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characterization_metadata_df = pd.DataFrame(characterization_params).set_index('row')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characterization_factors_defined_uncertainty = {}\n",
    "characterization_factors_undefined_uncertainty = []\n",
    "for characterization_matrix_index in reduced_characterization_matrix_ids:\n",
    "    characterization_metadata = characterization_metadata_df.loc[characterization_matrix_index]\n",
    "    if characterization_metadata['uncertainty_type']>0: # if the uncertainty type is larger than 0, meaining there is an uncertainty defined\n",
    "        characterization_factors_defined_uncertainty[characterization_matrix_index] = characterization_metadata.to_dict()\n",
    "    else: # if no uncertainty data is available\n",
    "        characterization_factors_undefined_uncertainty.append(characterization_matrix_index)\n",
    "print(\"Characterization factors with uncertainty information: {} \\nCharacterization factors without uncertainty information: {}\".format(len(characterization_factors_defined_uncertainty), len(characterization_factors_undefined_uncertainty)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Set the uncertainty concept for the undefined uncertainty parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stats-arrays.readthedocs.io/en/latest/\n",
    "\n",
    "![alt text](stats_arrays_table.JPG \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1. Set the uncertainty concept for the intervention flows and compute the bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the bounds of the uncertain intervention flows using `stats.arrays`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not intervention_flows_defined_uncertainty:\n",
    "    raise Exception('There are no intervention flows with defined uncertainty, we must implement another uncertainty approach in that case')\n",
    "if len(intervention_flows_defined_uncertainty) < len(intervention_flows_undefined_uncertainty):\n",
    "    raise Exception('There are more intervention flows with no defined uncertainty than defined uncertainty, this is a trivial measure set to warn the user if the averages bounds approach is useful')\n",
    "intervention_flow_defined_uncertainty_bounds = {}\n",
    "for intervention_flow_index, intervention_flow_uncertainty_dict in intervention_flows_defined_uncertainty.items():\n",
    "    intervention_flow_uncertainty_array = stats_arrays.UncertaintyBase.from_dicts(intervention_flow_uncertainty_dict)\n",
    "    uncertainty_choice = stats_arrays.uncertainty_choices[intervention_flow_uncertainty_dict['uncertainty_type']]\n",
    "    intervention_flow_statistics = uncertainty_choice.statistics(intervention_flow_uncertainty_array)\n",
    "    intervention_flow_defined_uncertainty_bounds[intervention_flow_index] = intervention_flow_statistics\n",
    "    intervention_flow_defined_uncertainty_bounds[intervention_flow_index]['amount'] = intervention_flow_uncertainty_dict['amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervention_flow_defined_uncertainty_bounds_df = pd.DataFrame(intervention_flow_defined_uncertainty_bounds).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if the bounds are valid upperbound > lowerbound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ((intervention_flow_defined_uncertainty_bounds_df['upper'] - intervention_flow_defined_uncertainty_bounds_df['lower']) <= 0).any():\n",
    "    raise Exception('There is one bound where the lower bound which is equal or larger than the upper bound')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the statistics of the lower and upper bounds returned from the parameter distributions to decide which metric to use for the flows which do not have uncertainty information\n",
    "\n",
    "**ATTN:**\n",
    "The choice of which value is use to subtract the upper and lower bound from makes a big difference. Previously I used the mean but that might weight outliers to much compared to the mode or the median. Then also all values have a \"amount\" which is the deterministic amount, which again can differ from any of first order moments. I have chosen the 'amount' here since this approach aims to find the upper and lower bound of parameters without uncertainty infromation which only have their deterministic amount, so computing the scaling coefficients based on the difference between amount and upper and lower bound might have the best genenerelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if intervention_flows_undefined_uncertainty:\n",
    "    lower_spread = (intervention_flow_defined_uncertainty_bounds_df['amount'] - intervention_flow_defined_uncertainty_bounds_df['lower']).abs() / intervention_flow_defined_uncertainty_bounds_df['amount'].abs()\n",
    "    upper_spread = (intervention_flow_defined_uncertainty_bounds_df['amount'] - intervention_flow_defined_uncertainty_bounds_df['upper']).abs() / intervention_flow_defined_uncertainty_bounds_df['amount'].abs()\n",
    "    ax = lower_spread.hist(bins=30, label='lower spread')\n",
    "    upper_spread.hist(bins=30, label='upper spread', ax=ax, alpha=0.5)\n",
    "    ax.legend()\n",
    "    print('upper spread statistics')\n",
    "    print('mean: {:.4f}\\nmode: {}\\nmedian: {:.4f}\\nstd: {:.4f}\\nmin: {:.4f}\\nmax: {:.4f}\\n'.format(upper_spread.mean(), upper_spread.mode(), upper_spread.median(), upper_spread.std(), upper_spread.min(), upper_spread.max()))\n",
    "    print('\\nlower spread statistics')\n",
    "    print('mean: {:.4f}\\nmode: {}\\nmedian: {:.4f}\\nstd: {:.4f}\\nmin: {:.4f}\\nmax: {:.4f}\\n'.format(lower_spread.mean(), lower_spread.mode(), lower_spread.median(), lower_spread.std(), lower_spread.min(), lower_spread.max()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on the statistics below, I decided to use the median of the amount fraction of the upper and lower bound, since the distribution of the spreads contains multiple modes and many \"outliers\" which will distort the mean greatly.\n",
    "\n",
    "**ATTN:**\n",
    "There are multiple modes in the spread statistics, which means there seems to be a few 'groups' or 'types' of intervention flows which have very different spreads, it might be good to analyze which these are to make the extrapolation more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if intervention_flows_undefined_uncertainty:\n",
    "    intervention_flow_upper_spread_scaling_factor = upper_spread.median()\n",
    "    intervention_flow_lower_spread_scaling_factor = lower_spread.median()\n",
    "    print('The upper spread scaling factor for intervention flows is: {}\\nThe lower spread scaling factor for intervention flows is: {}'.format(intervention_flow_upper_spread_scaling_factor, intervention_flow_lower_spread_scaling_factor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this the bounds of the intervention flows without uncertainty is computed\n",
    "\n",
    "**ATTN For negative flows the skewness might need to be inversed!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if intervention_flows_undefined_uncertainty:\n",
    "    intervention_flow_undefined_uncertainty_bounds = {}\n",
    "    for intervention_flow_index in intervention_flows_undefined_uncertainty:\n",
    "        amount = intervention_metadata_df.loc[intervention_flow_index].amount\n",
    "        # ATTN: BHL: If we have negative values than the skewdness which mostly is poisitve for positive flows will now be positive for negative flows (remain right skewed) while in reality negative flows might be left skewed (tail going away from zero not towards zero as now)\n",
    "        intervention_flow_uncertainty_dict = {\n",
    "            'amount': amount,\n",
    "            'upper': amount + intervention_flow_upper_spread_scaling_factor * abs(amount),\n",
    "            'lower': amount - intervention_flow_lower_spread_scaling_factor * abs(amount)\n",
    "        }\n",
    "        intervention_flow_undefined_uncertainty_bounds[intervention_flow_index] = intervention_flow_uncertainty_dict\n",
    "else:\n",
    "    intervention_flow_undefined_uncertainty_bounds = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if the bounds are valid: upper-bound > lower-bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if intervention_flows_undefined_uncertainty:\n",
    "    intervention_flow_undefined_uncertainty_bounds_df = pd.DataFrame(intervention_flow_undefined_uncertainty_bounds).T\n",
    "    if ((intervention_flow_undefined_uncertainty_bounds_df['upper'] - intervention_flow_undefined_uncertainty_bounds_df['lower']) <= 0).any():\n",
    "        raise Exception('There is one bound where the lower bound which is equal or larger than the upper bound')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There can be negative flows and their upper and lower bounds need to be considered in detail!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('invervention flows with negative median value:')\n",
    "intervention_flow_undefined_uncertainty_bounds_df[(intervention_flow_undefined_uncertainty_bounds_df['amount']) < 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2. Set the uncertainty concept for the characterization factors and compute the bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The characterization factors uncertainty depend strongly on the method chosen and also on the time horizon etc.\n",
    "\n",
    "**For now we are choosing an arbitrary bound scaling value of 0.3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characterization_factor_upper_scaling_factor = 0.15\n",
    "characterization_factor_lower_scaling_factor = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the bounds based on the scaling factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(characterization_factors_defined_uncertainty) > 0:\n",
    "    raise Exception('There are characterization factor uncertainty in the data, use these! (code not implemented yet, but analogueous to intervention flows)')\n",
    "\n",
    "characterization_factors_undefined_uncertainty_bounds = {}\n",
    "for characterization_factor_index in characterization_factors_undefined_uncertainty:\n",
    "    amount = characterization_metadata_df.loc[characterization_factor_index].amount\n",
    "    characterization_factor_uncertainty_dict = {\n",
    "        'amount': amount,\n",
    "        'lower': amount - (characterization_factor_lower_scaling_factor * abs(amount)),\n",
    "        'upper': amount + (characterization_factor_upper_scaling_factor * abs(amount))\n",
    "    }\n",
    "    characterization_factors_undefined_uncertainty_bounds[characterization_factor_index] = characterization_factor_uncertainty_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if the bounds are valid: upper-bound > lower-bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characterization_factors_undefined_uncertainty_bounds_df = pd.DataFrame(characterization_factors_undefined_uncertainty_bounds).T\n",
    "if ((characterization_factors_undefined_uncertainty_bounds_df['upper'] - characterization_factors_undefined_uncertainty_bounds_df['lower']) <= 0).any():\n",
    "    raise Exception('There is one bound where the lower bound which is equal or larger than the upper bound')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define the global sensitivity problem\n",
    "### 4.1. Define the bound/interval of the parameters\n",
    "Defining the bound as plus minus the standard deviation, $\\sigma^2$, of the nominal value, $\\mu$, for each parameter, $p$:\n",
    "\n",
    "$[\\mu_p - \\sigma^2_p; \\mu_p + \\sigma^2_p]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Defining the sampling problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bounds = intervention_flow_defined_uncertainty_bounds | intervention_flow_undefined_uncertainty_bounds | characterization_factors_undefined_uncertainty_bounds\n",
    "all_bounds_indx_dict = {\n",
    "    \"intervention_flows_end\": len(intervention_flow_defined_uncertainty_bounds | intervention_flow_undefined_uncertainty_bounds),\n",
    "    \"characterization_factors_start\":len(intervention_flow_defined_uncertainty_bounds | intervention_flow_undefined_uncertainty_bounds),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = {\n",
    "    'num_vars': len(all_bounds),\n",
    "    'names': list(all_bounds.keys()),\n",
    "    'bounds': [[bound['lower'], bound['upper']]for bound in all_bounds.values()]\n",
    "}\n",
    "print('problem includes:\\n{} uncertain intervention flows\\n{} uncertain characterization factors'.format(len(intervention_flow_defined_uncertainty_bounds | intervention_flow_undefined_uncertainty_bounds), len(characterization_factors_undefined_uncertainty_bounds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Define the sampling method\n",
    "\n",
    "Select the sampling method, any sampling method from SALib can be chosen, but for most sensitivity analysis there is a sampling method best suited or even neccessary, in this case we use `saltelli` sampling because it is best compatible with Sobol' sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SALib.sample import sobol as sample_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the amount of samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2**7\n",
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Define the sensitivity analysis method\n",
    "\n",
    "Select the sensitivity analysis method, remember it is mostly coupled to the sampling method, any sensitivity method from the SALib can be chosen. For this case study we chose Sobol' method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SALib.analyze import sobol as SA_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Perform the sensitivity analysis\n",
    "\n",
    "$$\n",
    "    e(Q, B) =  Q \\cdot B \\cdot s\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Sampling the $Q$ and $B$ arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = sample_method.sample(problem, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Solving for $Q \\cdot B \\cdot s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract matrizes from sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_intervention_flows = pd.DataFrame.sparse.from_spmatrix(\n",
    "    scipy.sparse.csr_matrix(sample_data[:,:all_bounds_indx_dict['intervention_flows_end']]), \n",
    "    columns=problem['names'][:all_bounds_indx_dict['intervention_flows_end']]\n",
    "    # columns=pd.MultiIndex.from_tuples(problem['names'][:all_bounds_indx_dict['intervention_flows_end']])\n",
    "    )\n",
    "sample_characterization_factors = pd.DataFrame.sparse.from_spmatrix(\n",
    "    scipy.sparse.csr_matrix(sample_data[:,all_bounds_indx_dict['characterization_factors_start']:]), \n",
    "    columns=problem['names'][all_bounds_indx_dict['characterization_factors_start']:]\n",
    "    )\n",
    "del sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the environmental costs $Q \\cdot B$ by reindexing the chracterization factors sample based on the intervention flow sample, so we can do dot product between for each characterization factors corresponding to the intervnetnion flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_index_intervention_flow = pd.DataFrame.from_records(sample_intervention_flows.columns.values)\n",
    "sample_characterization_factors_expanded = sample_characterization_factors.reindex(level_index_intervention_flow[0].values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_characterization_factors_expanded = sample_characterization_factors.reindex(sample_intervention_flows.columns, axis=1, level=0)\n",
    "# Set the columns values to match the characterization columns\n",
    "sample_intervention_flows.columns = level_index_intervention_flow[0].values\n",
    "sample_environmental_costs = sample_characterization_factors_expanded * sample_intervention_flows\n",
    "del sample_characterization_factors_expanded\n",
    "del sample_intervention_flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the environmental impact using a dot product of the reindex scaling vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the columns values to match the intervention columns\n",
    "sample_environmental_costs.columns = level_index_intervention_flow[1].values\n",
    "scaling_vector_expanded = result_data['scaling_vector']['Value'].reindex(level_index_intervention_flow[1].values)\n",
    "sample_characterized_inventories = sample_environmental_costs * scaling_vector_expanded\n",
    "sample_impacts = sample_environmental_costs @ scaling_vector_expanded\n",
    "del sample_environmental_costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Calculate the total output variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_impacts = sample_characterized_inventories.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if the bounds are valid: upper-bound > lower-bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if intervention_flow_undefined_uncertainty_bounds:\n",
    "    intervention_flow_undefined_uncertainty_bounds_df = pd.DataFrame(intervention_flow_undefined_uncertainty_bounds).T\n",
    "    if ((intervention_flow_undefined_uncertainty_bounds_df['upper'] - intervention_flow_undefined_uncertainty_bounds_df['lower']) <= 0).any():\n",
    "        raise Exception('There is one bound where the lower bound which is equal or larger than the upper bound')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The statistics of the the sample impacts: {method}')\n",
    "print(sample_impacts.sparse.to_dense().describe())\n",
    "print('The deterministic impact is {}'.format('\\n'.join(['{} : {:e}'.format(values[0], values[1]) for values in result_data['impacts'].values])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1.  Show the z-value and the distribution of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_impacts.plot.hist(bins=50)\n",
    "sample_impacts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The z-value of the total environmental impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_impacts.sparse.to_dense().std()/abs(sample_impacts.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Calculate Sobol index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Si_impacts = SA_method.analyze(problem, sample_impacts.sparse.to_dense().values, parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_Si_impacts, first_Si_impacts, second_Si_impacts = Si_impacts.to_df()\n",
    "total_Si_impacts = pd.DataFrame([Si_impacts['ST'].T, Si_impacts['ST_conf'].T], index=['ST', 'ST_conf'], columns=problem['names']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.1. Calculate total explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total explained variance is \\n{:.4}%\".format(total_Si_impacts[\"ST\"].sum()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Select the uncertain parameters with the largest explained variane\n",
    "\n",
    "Select the uncertain parameters using a cut-off value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_total_explained_variance = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_total_Si_impacts = total_Si_impacts.sort_values(\"ST\", ascending=False)[total_Si_impacts[\"ST\"]>cutoff_total_explained_variance]\n",
    "print('{} uncertain parameters remain after using a cutoff value of {}% for the total variance.'.format(reduced_total_Si_impacts.shape[0], cutoff_total_explained_variance*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6. Plot the contribution to variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the data and the names for the contribution plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_SA_barplot(data:pd.DataFrame, metadata:pd.DataFrame, colormap:pd.Series=pd.Series([]), bbox_to_anchor_lower:float = -0.6, bbox_to_anchor_center:float=0.5):\n",
    "    \"\"\"\n",
    "        Barplot of the contributional variance of the parameters in an objective\n",
    "\n",
    "        args:\n",
    "            data:       dataframe with columns: \"ST\" and \"ST_conf\"\n",
    "            metadata:   metadataframe with \"bar_names\" column and same indices as data\n",
    "            colormap:   Series with color codes to each data index     `colormap = pd.Series(mpl.cm.tab20.colors[:data.shape[0]], index=data.index)`\n",
    "            bbox_to_anchor_lower: negative float, scaled how much the legend is under the plot\n",
    "    \"\"\"\n",
    "    # width = 180\n",
    "    # height = 180\n",
    "    width = 4.77*72.4#600\n",
    "    height = None\n",
    "    _, ax = plt.subplots(1, 1, figsize=set_size(width,height))\n",
    "\n",
    "    # Data\n",
    "    data = data.sort_values([\"ST\"], ascending=False)\n",
    "    heights = data[\"ST\"].values * 100\n",
    "    yerrs = data[\"ST_conf\"].values * 100\n",
    "    bars = [textwrap.fill(string, 50) for string in metadata[\"bar_names\"].reindex(data.index)]\n",
    "    y_pos = range(len(bars))\n",
    "    \n",
    "    for height, y_po, yerr, indx in zip(heights, y_pos, yerrs, data.index):\n",
    "        ax.bar(y_po, height, yerr=yerr, capsize=5, ecolor=\"gray\", color=colormap[indx], alpha=0.9)\n",
    "    ax.set_xticks([])\n",
    "    if (data[\"ST\"]<=1).all() and (data[\"ST\"]>=0).all():\n",
    "        ax.yaxis.set_major_formatter(mpl.ticker.PercentFormatter())\n",
    "        ax.yaxis.set_major_locator(mpl.ticker.MultipleLocator(10))\n",
    "        # For the minor ticks, use no labels; default NullFormatter.\n",
    "        ax.yaxis.set_minor_locator(mpl.ticker.MultipleLocator(5))\n",
    "    ax.legend(bars, loc='lower center', bbox_to_anchor=(bbox_to_anchor_center, bbox_to_anchor_lower), borderpad=1)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.yaxis.grid(color='gray', linestyle='dotted')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dict = {}\n",
    "for (intervention_index, process_index) in total_Si_impacts.index[:all_bounds_indx_dict['intervention_flows_end']]:\n",
    "    metadata_dict[(intervention_index, process_index)] = '{} --- {}'.format(pulpo_worker.lci_data['process_map_metadata'][process_index], pulpo_worker.lci_data['intervention_map_metadata'][intervention_index])\n",
    "for intervention_index in total_Si_impacts.index[all_bounds_indx_dict['characterization_factors_start']:]:\n",
    "    metadata_dict[intervention_index] = '{} --- {}'.format(pulpo_worker.lci_data['intervention_map_metadata'][intervention_index], method)\n",
    "metadata_total_Si_impacts = pd.DataFrame([metadata_dict], index=['bar_names']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_Si_impacts_top10 = total_Si_impacts.sort_values('ST', ascending=False).iloc[:10,:]\n",
    "metadata_total_Si_impacts_top10 = metadata_total_Si_impacts.loc[total_Si_impacts_top10.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap_base = mpl.cm.tab20.colors\n",
    "colormap_SA_barplot = pd.Series(colormap_base[:total_Si_impacts_top10.shape[0]], index=total_Si_impacts_top10.index)\n",
    "plot_SA_barplot(data=total_Si_impacts_top10, metadata=metadata_total_Si_impacts_top10, colormap=colormap_SA_barplot, bbox_to_anchor_center=1.7, bbox_to_anchor_lower=-.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7. Plot the main contributing variables to the total environmental impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_QBs = sample_impacts\n",
    "QBs_per_s_sample = sample_characterized_inventories\n",
    "QBs_per_s_sample.columns = pd.MultiIndex.from_frame(level_index_intervention_flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QBs_tops_indcs = QBs_per_s_sample.mean().abs().sort_values(ascending=False).iloc[:10].index\n",
    "data_QBs = pd.DataFrame([])\n",
    "QBs_per_s_sample_scaled = QBs_per_s_sample/ e_QBs.mean()#.divide(QBs_per_s_sample.abs().sum(axis=1), axis=\"index\")\n",
    "data_QBs[\"ST\"] = QBs_per_s_sample_scaled.mean()[QBs_tops_indcs]\n",
    "data_QBs[\"ST_conf\"] = QBs_per_s_sample_scaled.sparse.to_dense().std()[QBs_tops_indcs]\n",
    "data_QBs.index = data_QBs.index.to_flat_index()\n",
    "metadata_QBs = metadata_total_Si_impacts.loc[data_QBs.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the total environmental impact for the top processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "# set matplotlib colormap\n",
    "\n",
    "def plot_total_env_impact_barplot(data:pd.DataFrame,  metadata:pd.DataFrame, impact_category:str, colormap_base:tuple, colormap_linked:pd.Series=pd.Series([]), savefig:Optional[bool]=None, bbox_to_anchor_center:float=0.7, bbox_to_anchor_lower:float=0.7):\n",
    "    \"\"\"\n",
    "        Barplot of the contributional variance of the parameters in total cost objective\n",
    "\n",
    "        args:\n",
    "            data:       dataframe with columns: \"ST\" and \"ST_conf\"\n",
    "            metadata:   metadataframe with \"bar_names\" column and same indices as data\n",
    "            impact_category:    name of environmental impact category\n",
    "            colormap_base:      The colormap which should be used for the plot, use the same as underlying the colormap_linked if it is specified\n",
    "            colormap_linked:    If there is a colormap from another plot where the variables shown in this plot should refer to if they appear in both\n",
    "            savefig:    if true saves fig into specified path\n",
    "    \"\"\"\n",
    "    if colormap_linked.empty:\n",
    "        colormap = pd.Series(colormap_base[:data.shape[0]], index=data.index)\n",
    "    else:\n",
    "        # act_indcs = [index for index in colormap.index if type(index[1]) == int]\n",
    "        # colormap_red = pd.Series(colormap[act_indcs].values, index=[indcs[1] for indcs in act_indcs])\n",
    "        colormap_red = colormap_linked.loc[colormap_linked.index.isin(data.index)]\n",
    "        addtional_incs = data.index[~data.index.isin(colormap_red.index)]\n",
    "        additional_colormap = pd.Series(\n",
    "            colormap_base[colormap_linked.shape[0]:colormap_linked.shape[0]+len(addtional_incs)], \n",
    "            index = addtional_incs\n",
    "            )\n",
    "        colormap = pd.concat([colormap_red, additional_colormap])\n",
    "\n",
    "\n",
    "    ax = plot_SA_barplot(data, metadata, colormap=colormap, bbox_to_anchor_lower=bbox_to_anchor_lower, bbox_to_anchor_center=bbox_to_anchor_center)    \n",
    "    ax.set_xlabel(\"Main environmental parameters\")\n",
    "    ax.set_ylabel(\"Contribution to total {} in [\\%]\".format(impact_category))\n",
    "\n",
    "    # Save figure\n",
    "    if savefig:\n",
    "        plt.savefig(r\"C:\\Users\\admin\\OneDrive - Carbon Minds GmbH\\Dokumente\\13 Students\\MA_Bartolomeus_Löwgren\\02_code\\03_optimization_framework\\04_case_studies\\02_plots\\total_env_impact_barplot\" + \".{}\".format(fileformat), format=fileformat, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = pd.Series(mpl.cm.tab20.colors[:data_QBs.shape[0]], index=data_QBs.index)\n",
    "metadata_QBs = metadata_QBs[['bar_names']]\n",
    "plot_total_env_impact_barplot(data_QBs, metadata=metadata_QBs, impact_category=method, colormap_base=colormap_base, colormap_linked=colormap_SA_barplot, savefig=False, bbox_to_anchor_center=1.7, bbox_to_anchor_lower=-.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CC formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Extract the metadata for the selected uncertain parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.1. If the uncertain parameters are returned by the GSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_uncertain_intervention_flow_indc = [index for index in reduced_total_Si_impacts.index if type(index)==tuple]\n",
    "selected_uncertain_characterization_factor_indc = [index for index in reduced_total_Si_impacts.index if type(index)!=tuple]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.2. If the uncertain parameters are returned from the amount reduction (without GSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_uncertain_intervention_flow_indc = intervention_flow_defined_uncertainty_bounds_df.index.to_list() + intervention_flow_undefined_uncertainty_bounds_df.index.to_list()\n",
    "selected_uncertain_characterization_factor_indc = characterization_factors_undefined_uncertainty_bounds_df.index.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.3. Extract the uncertainty data from the metadata extracted from the lci_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_uncertain_intervention_flow_metadata_df = intervention_metadata_df.loc[selected_uncertain_intervention_flow_indc]\n",
    "selected_uncertain_characterization_fact_metadata_df = characterization_metadata_df.loc[selected_uncertain_characterization_factor_indc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Transform all parametric uncertainty to normal distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.1. Transform missing uncertainty information via bounds to normal distributions\n",
    "\n",
    "When there is no uncertainty information at all, as the case for the characterization factors, then we take the assumption about the bounds and turn them into distribution information.\n",
    "\n",
    "Probability distribution assumptions:\n",
    "- Normal distributed uncertain parameters\n",
    "- The two bounds enclose a 95% confidence interval\n",
    "\n",
    "This allows us to calculate the standard distribution, since 95% confidence interval is equal to two standard normal distributions.\n",
    "\n",
    "$$\n",
    "    upperbound - lowerbound = 4 \\cdot \\sigma\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2.1.1. For the characterization factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_uncertain_characterization_fact_metadata_df = selected_uncertain_characterization_fact_metadata_df.copy()\n",
    "if (selected_uncertain_characterization_fact_metadata_df['uncertainty_type'] == 0).any():\n",
    "    for param_index in final_uncertain_characterization_fact_metadata_df.index[final_uncertain_characterization_fact_metadata_df['uncertainty_type'] == 0]:\n",
    "        final_uncertain_characterization_fact_metadata_df.loc[param_index, 'scale'] = (characterization_factors_undefined_uncertainty_bounds_df.loc[param_index, 'upper'] - characterization_factors_undefined_uncertainty_bounds_df.loc[param_index,'lower']) / 4\n",
    "        final_uncertain_characterization_fact_metadata_df.loc[param_index, 'uncertainty_type'] = 3\n",
    "print('{} characterization factors with no uncertainty information were transformed into normal distributions via their priorly interpolated bounds'.format((selected_uncertain_characterization_fact_metadata_df['uncertainty_type'] == 0).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2.1.2. For the intervention flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_uncertain_intervention_flow_metadata_df = selected_uncertain_intervention_flow_metadata_df.copy()\n",
    "if (selected_uncertain_intervention_flow_metadata_df['uncertainty_type'] == 0).any():\n",
    "    for param_index in final_uncertain_intervention_flow_metadata_df.index[final_uncertain_intervention_flow_metadata_df['uncertainty_type'] == 0]:\n",
    "        final_uncertain_intervention_flow_metadata_df.loc[param_index, 'scale'] = (intervention_flow_undefined_uncertainty_bounds_df.loc[param_index, 'upper'] - intervention_flow_undefined_uncertainty_bounds_df.loc[param_index,'lower']) / 4\n",
    "        final_uncertain_intervention_flow_metadata_df.loc[param_index, 'uncertainty_type'] = 3\n",
    "print('{} intervention flows with no uncertainty information were transformed into normal distributions via their priorly interpolated bounds'.format((selected_uncertain_intervention_flow_metadata_df['uncertainty_type'] == 0).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.2. Transform all non normal distribution via sampling and fitting to normal distributions\n",
    "\n",
    "For all lognormal (and all other distributions) distributions fit a normal distribution to sampled data to find the best normal approximation of the lognormal distributions. We use sample sizes of 1.000.000 to more the likelihood estimation of the normal distribution to the lognoramlly sampled data as good as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distributions = False\n",
    "sample_size = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} intervention flows with lognormal uncertainty information were transformed into normal distributions via max likelihood approximation'.format((final_uncertain_intervention_flow_metadata_df['uncertainty_type'] == 2).sum()))\n",
    "for param_index, metadata in final_uncertain_intervention_flow_metadata_df[final_uncertain_intervention_flow_metadata_df['uncertainty_type'] == 2].iterrows():\n",
    "    if metadata['uncertainty_type'] == 1:\n",
    "        raise Exception('The intervention flow has the \"no uncertainty\" distribution type. This is not allowed')\n",
    "    metadata_uncertainty_array = stats_arrays.UncertaintyBase.from_dicts(metadata.to_dict())\n",
    "    uncertainty_choice = stats_arrays.uncertainty_choices[metadata['uncertainty_type']]\n",
    "    # Sample the lognormal distribution \n",
    "    param_samples = uncertainty_choice.random_variables(metadata_uncertainty_array, sample_size)\n",
    "    # Calculate the ppf values to \n",
    "    percentages = np.expand_dims(np.linspace(0.001, 0.999, 1000, axis=0), axis=0)\n",
    "    x = uncertainty_choice.ppf(metadata_uncertainty_array, percentages=percentages)\n",
    "    x, y = uncertainty_choice.pdf(metadata_uncertainty_array, xs=x)\n",
    "    # Fit a normal distribution to the sampled data\n",
    "    loc_norm, scale_norm = scipy.stats.norm.fit(param_samples.T)\n",
    "    if plot_distributions:\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        # plot the histrogram of the samples\n",
    "        ax.hist(param_samples.T, density=True, bins='auto', histtype='stepfilled', alpha=0.2, label='{} samples'.format(uncertainty_choice.description))\n",
    "        # plot the lognormal pdf\n",
    "        ax.plot(x, y, 'k-', lw=2, label='frozen {} pdf'.format(uncertainty_choice.description))\n",
    "        # Plot the fitted normal distibution\n",
    "        ax.plot(x, scipy.stats.norm.pdf(x,  loc=loc_norm, scale=scale_norm), 'b-', lw=2, label='fitted normal pdf')\n",
    "        ax.set_title(str(param_index))\n",
    "        ax.legend(loc='best', frameon=False)\n",
    "    # Overwrite the lognormal distribution statistics with the fitted normal \n",
    "    final_uncertain_intervention_flow_metadata_df.loc[param_index, 'scale'] = scale_norm\n",
    "    final_uncertain_intervention_flow_metadata_df.loc[param_index, 'loc'] = loc_norm\n",
    "    final_uncertain_intervention_flow_metadata_df.loc[param_index, 'uncertainty_type'] = 3\n",
    "if plot_distributions:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3. Check all the standard deviation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNcertainty in the intervention flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_uncertain_intervention_flow_metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncertainty in the characterization factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_uncertain_characterization_fact_metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Compute the envrionmental cost standard deviations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data extraction\n",
    "    - Get all the invervention flows to the uncertain characterization factor\n",
    "\n",
    "2. Data calculation\n",
    "    - compute the environmental cost standard deviation for every environmental cost factor which contain one of the uncertain intervention flows or characterization factors characterization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.1. Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the variance of the environmental costs for all processes which contain\n",
    "# - an uncertain intervention flow\n",
    "process_id_uncertain_inv_flow = final_uncertain_intervention_flow_metadata_df.index.get_level_values(1).values\n",
    "# - an intervention flow associated with an uncertain characterization factor\n",
    "process_id_associated_char_factor = pulpo_worker.lci_data['intervention_matrix'][final_uncertain_characterization_fact_metadata_df.index,:].nonzero()[1]\n",
    "process_ids = np.unique(np.append(process_id_associated_char_factor, process_id_uncertain_inv_flow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the intervention flows to the uncertain characterization factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervention_flows_extracted = pd.DataFrame.sparse.from_spmatrix(\n",
    "    pulpo_worker.lci_data['intervention_matrix'][final_uncertain_characterization_fact_metadata_df.index.values,:][:,process_ids],\n",
    "    index=final_uncertain_characterization_fact_metadata_df.index,\n",
    "    columns=process_ids\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.2. Data calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the standard deviation of the environmental costs using the uncertain intervention flows and characterization factors\n",
    "\n",
    "$$\n",
    "\n",
    "\\sigma_{q_hb_j} =\\sqrt{\\sum_e \\big(\\mu_{q_{h,e}}^2\\sigma_{b_{e,j}}^2 + \\mu_{b_{e,j}}^2\\sigma_{q_{h,e}}^2 + \\sigma_{b_{e,j}}^2 \\sigma_{q_{h,e}}^2\\big)}\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environmental_costs_std = {}\n",
    "for process_id in process_ids:\n",
    "    # compute the mu_{q_{h,e}}^2 * sigma_{b_{e,j}}^2\n",
    "    if process_id in final_uncertain_intervention_flow_metadata_df.index.get_level_values(level='col'):\n",
    "        intervention_flow_std = final_uncertain_intervention_flow_metadata_df.xs(process_id, level='col', axis=0, drop_level=True)['scale']\n",
    "        characterization_factor_mean = pd.Series(\n",
    "            pulpo_worker.lci_data[\"matrices\"][method].diagonal()[\n",
    "                intervention_flow_std.index.get_level_values(level='row')\n",
    "                ],\n",
    "            index=intervention_flow_std.index.get_level_values(level='row')\n",
    "        )\n",
    "        # Reindex so that we can perform a matrix multiplication on all intervention flows\n",
    "        characterization_factor_mean = characterization_factor_mean.reindex(intervention_flow_std.index, axis=0, level='row')\n",
    "        mu_q2_sigma_b2 = characterization_factor_mean.pow(2).mul(intervention_flow_std.pow(2), axis=0)\n",
    "    else:\n",
    "        mu_q2_sigma_b2 = pd.Series([0])\n",
    "    # compute the mu_{b_{e,j}}^2 * sigma_{q_{h,e}}^2\n",
    "    if (intervention_flows_extracted[process_id] > 0).any():\n",
    "        characterization_factor_std = final_uncertain_characterization_fact_metadata_df['scale']\n",
    "        # Reindex so that we can perform a matrix multiplication on all characterization factors\n",
    "        intervention_flow_mean = intervention_flows_extracted[process_id]\n",
    "        sigma_q2_mu_b2 = characterization_factor_std.pow(2).mul(intervention_flow_mean.pow(2), axis=0)\n",
    "    else:\n",
    "        sigma_q2_mu_b2 = pd.Series([0])\n",
    "    # compute the sigma_{b_{e,j}}^2 * sigma_{q_{h,e}}^2\n",
    "    if (intervention_flows_extracted[process_id] > 0).any() and process_id in final_uncertain_intervention_flow_metadata_df.index.get_level_values(level='col'):\n",
    "        sigma_q2_sigma_b2 = characterization_factor_std.pow(2).mul(intervention_flow_std.pow(2))\n",
    "    else:\n",
    "        sigma_q2_sigma_b2 = pd.Series([0])\n",
    "    # Take the sqrt of the sum over the std terms and the intervention flows\n",
    "    environmental_costs_std[process_id] = np.sqrt(mu_q2_sigma_b2.sum() + sigma_q2_sigma_b2.sum() + sigma_q2_mu_b2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to track the very large std and mean differences in env cost\n",
    "\n",
    "# pulpo_worker.lci_data[\"matrices\"][method].diagonal()[pulpo_worker.lci_data['intervention_matrix'][:,process_id].nonzero()[0]]\n",
    "# pulpo_worker.lci_data['intervention_matrix'][:,process_id][pulpo_worker.lci_data['intervention_matrix'][:,process_id].nonzero()]\n",
    "# pulpo_worker.lci_data['intervention_matrix'][:,process_id].nonzero()\n",
    "# pulpo_worker.lci_data['intervention_map_metadata'][82]\n",
    "# pulpo_worker.lci_data['process_map_metadata'][process_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the mean of the environmental costs to be used together with the standard deviation to update the uncertain parameters in line with chance constraint formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environmental_cost_mean = pulpo_worker.lci_data[\"matrices\"][method].diagonal() @ pulpo_worker.lci_data['intervention_matrix']\n",
    "# environmental_cost_mean = dict(zip(range(len(environmental_cost_mean)), environmental_cost_mean))\n",
    "environmental_cost_mean = {env_cost_index[0]: env_cost for env_cost_index, env_cost in result_data['ENV_COST_MATRIX']['ENV_COST_MATRIX'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTN: For the environmental costs with very large z-value we should check if they come from interpolated values or from database uncertainty\n",
    "environmental_costs_std_mean = pd.DataFrame.from_dict(environmental_costs_std, orient='index', columns=['std'])\n",
    "environmental_costs_std_mean['metadata'] = environmental_costs_std_mean.index.map(pulpo_worker.lci_data['process_map_metadata'])\n",
    "if environmental_costs_std_mean['std'].isna().any():\n",
    "    raise Exception('There are NaNs in the standard deviation')\n",
    "environmental_costs_std_mean['mean'] = environmental_costs_std_mean.index.map(environmental_cost_mean)\n",
    "environmental_costs_std_mean['z'] = environmental_costs_std_mean['std'] / environmental_costs_std_mean['mean']\n",
    "if (environmental_costs_std_mean['z'] > 0.5).any():\n",
    "    print('These environmental costs have a standard deviation larger than 50% of their mean:\\n')\n",
    "    display(environmental_costs_std_mean[environmental_costs_std_mean['z'] > 0.5].sort_values('z', ascending=False))\n",
    "    # raise Exception('There are z-values greater than 0.5 this is improbable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environmental_costs_std_mean['z'].plot.box()\n",
    "environmental_costs_std_mean['z'].sort_values(ascending=False).iloc[5:].plot.box()\n",
    "print('The following points were excluded from the boxplot:')\n",
    "display(environmental_costs_std_mean['z'].sort_values(ascending=False).iloc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 6.4. Impose chance constraint formulation\n",
    "\n",
    "Data manipulation:\n",
    "- manipulate the environmental cost by adding the product of the PDF of the probability level and the standard deviation of the environmental costs\n",
    "- exchange the updated enviornmental costs in the environmental cost matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_QB = 0.9\n",
    "ppf_lambda_QB = scipy.stats.norm.ppf(lambda_QB)\n",
    "environmental_cost_updated = {}\n",
    "for process_id, environmental_cost in environmental_costs_std.items():\n",
    "    pulpo_worker.instance.ENV_COST_MATRIX[(process_id, method)] = environmental_cost_mean[process_id] + ppf_lambda_QB * environmental_costs_std[process_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.1. Solve the chance constrained problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulpo_worker.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data = extract_results(pulpo_worker.instance, pulpo_worker.project, pulpo_worker.database, choices, {}, demand,\n",
    "                            pulpo_worker.lci_data['process_map'], pulpo_worker.lci_data['process_map_metadata'],\n",
    "                            pulpo_worker.lci_data['intervention_map'], pulpo_worker.lci_data['intervention_map_metadata'],\n",
    "                            pulpo_worker.directory, \"\")\n",
    "# pulpo.saver.summarize_results(result_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Copmute the standard deviation and mean of the environmental cost flows\n",
    "- Look into how the L2 and L1 norm differ for the impact calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5. Solve the CC problem for a set of probability levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data_CC = {}\n",
    "lambda_array = np.linspace(0.5,1, 5, endpoint=False)\n",
    "for lambda_QB in lambda_array:\n",
    "    print(f'solving CC problem for lambda_QB = {lambda_QB}')\n",
    "    ppf_lambda_QB = scipy.stats.norm.ppf(lambda_QB)\n",
    "    environmental_cost_updated = {(process_id, method): environmental_cost_mean[process_id] + ppf_lambda_QB * environmental_costs_std[process_id] for process_id in environmental_costs_std.keys()}\n",
    "    pulpo_worker.instance.ENV_COST_MATRIX.store_values(environmental_cost_updated, check=True)\n",
    "    # for process_id, environmental_cost in environmental_costs_std.items():\n",
    "    #     updated_environmental_cost = environmental_cost_mean[process_id] + ppf_lambda_QB * environmental_costs_std[process_id]\n",
    "    #     pulpo_worker.instance.ENV_COST_MATRIX[(process_id, method)].set_value(updated_environmental_cost)\n",
    "    pulpo_worker.solve()\n",
    "\n",
    "    result_data_CC[lambda_QB] = extract_results(pulpo_worker.instance, pulpo_worker.project, pulpo_worker.database, choices, {}, demand,\n",
    "                                pulpo_worker.lci_data['process_map'], pulpo_worker.lci_data['process_map_metadata'],\n",
    "                                pulpo_worker.lci_data['intervention_map'], pulpo_worker.lci_data['intervention_map_metadata'],\n",
    "                                pulpo_worker.directory, \"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.5.1. Look into how the probability levels influence the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total impact of the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_CC_pareto_solution_bar_plots(data:pd.DataFrame, y_label:str, bbox_to_anchor:tuple=(1.40, .05)):\n",
    "    \"\"\"\n",
    "        args:\n",
    "            data:       columns: lambdas, rows: QBs grouped by something\n",
    "    \"\"\"\n",
    "    \n",
    "    # set figure and plot\n",
    "    width = 6\n",
    "    height = 6\n",
    "    _, ax = plt.subplots(1, 1, figsize=(width,height))\n",
    "\n",
    "    data_cleaned = data.copy()\n",
    "    data_cleaned_scaled = data_cleaned.abs().divide(data_cleaned.abs().sum())\n",
    "    data_cumsum = data_cleaned_scaled.cumsum(axis=0)\n",
    "    width = .8\n",
    "    labels = [\"{:.3f}\".format(label) for label in data.columns.astype(float).values]\n",
    "    bottom_data = np.zeros(len(labels))\n",
    "    for i_row, (type, row_data) in enumerate(data_cumsum.iterrows()):\n",
    "        ax.bar(labels, row_data.values-bottom_data, width, bottom=bottom_data, label=type, color=mpl.cm.tab20.colors[i_row])\n",
    "        bottom_data = row_data.values\n",
    "    ax.axhline(y=0, color='k')\n",
    "    ax.set_xlabel(\"probability level ($\\lambda$)\")\n",
    "    ax.set_ylabel(\"{} in [\\%]\".format(y_label))\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(45)\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(labels, data.sum().values/1e9, \"kx-\", label=\"total GWP\", linewidth=1)\n",
    "    ax2.set_ylabel(y_label)\n",
    "\n",
    "    lines, labels = ax.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax.legend(lines + lines2, labels + labels2, loc='lower center', bbox_to_anchor=bbox_to_anchor, borderpad=1, facecolor=\"None\")\n",
    "    ax.set_facecolor(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impacts = {}\n",
    "print(method)\n",
    "for lambda_QB, result_data in result_data_CC.items():\n",
    "    impacts[lambda_QB] = result_data['impacts'].set_index('Key').loc[method,'Value']\n",
    "    print('{}: {}'.format(lambda_QB, impacts[lambda_QB]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The changs in the choices of the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices_results = {}\n",
    "for i_CC, (lambda_QB, result_data) in enumerate(result_data_CC.items()):\n",
    "    for choice in choices.keys():\n",
    "        if i_CC == 0:\n",
    "            choices_results[choice] = result_data['choices'][choice][['Process', 'Capacity']].dropna()\n",
    "        choices_results[choice] = choices_results[choice].join(result_data['choices'][choice]['Value'].rename(lambda_QB), how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for choice, choice_result in choices_results.items():\n",
    "    print(choice)\n",
    "    display(choice_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes in the scaling vector and the characterized and scaled inventories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lambda_1, lambda_2 in zip(lambda_array[:len(lambda_array)-1], lambda_array[1:len(lambda_array)]):\n",
    "    print(f'lambda_1: {lambda_1}\\nlambda_2: {lambda_2}\\n')\n",
    "    scaling_vector_diff = ((result_data_CC[lambda_1]['scaling_vector'].set_index('ID')['Value'] - result_data_CC[lambda_2]['scaling_vector'].set_index('ID')['Value']))\n",
    "    scaling_vector_ratio = (scaling_vector_diff / result_data_CC[lambda_1]['scaling_vector'].set_index('ID')['Value']).abs().sort_values(ascending=False)\n",
    "    environmental_cost_mean = {env_cost_index[0]: env_cost for env_cost_index, env_cost in result_data_CC[lambda_1]['ENV_COST_MATRIX']['ENV_COST_MATRIX'].items()}\n",
    "    characterized_scaling_vector_diff = (scaling_vector_diff * pd.Series(environmental_cost_mean).reindex(scaling_vector_diff.index)).abs()\n",
    "    characterized_scaling_vector_diff_relative = (characterized_scaling_vector_diff / result_data_CC[lambda_1]['impacts'].set_index('Key').loc[method, 'Value']).abs().sort_values(ascending=False)\n",
    "\n",
    "    print('Amount of process scaling variables that changed:\\n{}: >1% \\n{}: >10%\\n{}: >100%\\n{}: >1000%\\n'.format((scaling_vector_ratio > 0.01).sum(), (scaling_vector_ratio > 0.1).sum(), (scaling_vector_ratio > 1).sum(), (scaling_vector_ratio > 10).sum()))\n",
    "    print('Amount of process characterized scaling variables (impacts per process) that changed:\\n{}: >1% \\n{}: >10%\\n{}: >100%\\n{}: >1000%\\n'.format((characterized_scaling_vector_diff_relative > 0.01).sum(), (characterized_scaling_vector_diff_relative > 0.1).sum(), (characterized_scaling_vector_diff_relative > 1).sum(), (characterized_scaling_vector_diff_relative > 10).sum()))\n",
    "    print('{:.5e}: is the maximum impact change in one process\\n{:.5e}: is the total impact change\\n'.format(characterized_scaling_vector_diff_relative.max(), characterized_scaling_vector_diff_relative.sum()))\n",
    "\n",
    "    amount_of_rows_for_visiualization = 10\n",
    "    print('The relative change of the scaling vector (s_lambda_1 - s_lambda_2)/s_lambda_1:\\n')\n",
    "    print(scaling_vector_ratio.iloc[:amount_of_rows_for_visiualization].rename(result_data_CC[lambda_2]['scaling_vector'].set_index('ID')['Process metadata']).sort_values(ascending=False))\n",
    "    print('\\n---\\n')\n",
    "    print('The relative change of the characterized scaling vector (s_lambda_1 - s_lambda_2)*QB_s / QBs:\\n')\n",
    "    print(characterized_scaling_vector_diff_relative.iloc[:amount_of_rows_for_visiualization].rename(result_data_CC[lambda_2]['scaling_vector'].set_index('ID')['Process metadata']))\n",
    "    print('\\n---\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_value = 0.002\n",
    "data_QBs_list = []\n",
    "for lamnda_QBs, result_data in result_data_CC.items():\n",
    "    environmental_cost_mean = {env_cost_index[0]: env_cost for env_cost_index, env_cost in result_data_CC[lamnda_QBs]['ENV_COST_MATRIX']['ENV_COST_MATRIX'].items()}\n",
    "    QBs = result_data['scaling_vector'].set_index('ID')['Value'] * pd.Series(environmental_cost_mean).reindex(result_data['scaling_vector']['Value'].index)\n",
    "    QBs_main = QBs[QBs.abs() > cutoff_value*QBs.abs().sum()]\n",
    "    QBs_main.name = lamnda_QBs\n",
    "    data_QBs_list.append(QBs_main)\n",
    "    print('With a cutoff value of {}, we keep {} process to an error of {:.2%}'.format(cutoff_value, len(QBs_main), abs(1 - QBs_main.sum()/QBs.sum())))\n",
    "data_QBs = pd.concat(data_QBs_list, axis=1)\n",
    "data_QBs = data_QBs.rename(index={process_id: pulpo_worker.lci_data['process_map_metadata'][process_id] for process_id in data_QBs.index})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_to_anchor = (0.65, -1.)\n",
    "plot_CC_pareto_solution_bar_plots(data_QBs, method, bbox_to_anchor=bbox_to_anchor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "- Extract environmental costs from pyomo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pulpo_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
